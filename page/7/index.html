<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>笔记本</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="笔记本">
<meta property="og:url" content="http://xiaoyue26.github.io/page/7/index.html">
<meta property="og:site_name" content="笔记本">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="风梦七">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="笔记本" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  
<link rel="stylesheet" href="/css/style.css">

  

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://xiaoyue26.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">笔记本</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">风梦七</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-2019-07/粗俗理解clickhouse" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/06/2019-07/%E7%B2%97%E4%BF%97%E7%90%86%E8%A7%A3clickhouse/" class="article-date">
  <time datetime="2019-07-06T09:06:37.000Z" itemprop="datePublished">2019-07-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/clickhouse/">clickhouse</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/06/2019-07/%E7%B2%97%E4%BF%97%E7%90%86%E8%A7%A3clickhouse/">粗俗理解clickhouse</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="what-clickhouse是啥"><a href="#what-clickhouse是啥" class="headerlink" title="what: clickhouse是啥?"></a>what: clickhouse是啥?</h1><p>clickhouse是俄罗斯开源的一个用于OLAP分析的核心引擎，它可以基于海量的日志数据接受类sql查询，以秒~分钟量级的延迟返回查询结果。<br>它目前应用在了俄罗斯的搜索引擎Yandex.Metrica中、欧洲核子研究中心: PB级存储、统计分析查询，以及我国各大互联网公司的BI后台引擎中。</p>
<h2 id="应用：-Yandex-Metrica"><a href="#应用：-Yandex-Metrica" class="headerlink" title="应用： Yandex.Metrica"></a>应用： Yandex.Metrica</h2><p>2014年: 每天120亿个事件。（点击、浏览）<br>374台服务器，20.3万亿行数据。<br>压缩后: 2PB<br>压缩前: 17PB</p>
<p>详细介绍官网:<br><a href="https://clickhouse.yandex/docs/zh/" target="_blank" rel="noopener">https://clickhouse.yandex/docs/zh/</a><br>开源代码:<br><a href="https://github.com/yandex/ClickHouse" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse</a><br>中文文档:<br><a href="https://github.com/yandex/ClickHouse/tree/master/docs/zh" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse/tree/master/docs/zh</a></p>
<h1 id="why-为啥选择clickhouse"><a href="#why-为啥选择clickhouse" class="headerlink" title="why: 为啥选择clickhouse?"></a>why: 为啥选择clickhouse?</h1><p>主要原因有: 性能高、跑分高、功能多、可用性高。</p>
<h2 id="性能高、跑分高"><a href="#性能高、跑分高" class="headerlink" title="性能高、跑分高"></a>性能高、跑分高</h2><p>// 俄罗斯的程序员在算法方面的活跃度排名世界第一<br>// C++实现、老毛子轻易不开源,参见nginx</p>
<blockquote>
<p>摘自知乎: <a href="https://zhuanlan.zhihu.com/p/22165241" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22165241</a><br>1亿数据:<br>比Vertica快5倍,比Hive快279倍,比Mysql快801倍;<br>10亿数据:<br>比Vertica快5倍,mysql无法完成。</p>
</blockquote>
<h3 id="单机性能"><a href="#单机性能" class="headerlink" title="单机性能"></a>单机性能</h3><p>有page cache: 2-10GB／s（未压缩），上限30GB/s<br>无page cache: 1.2G/s(压缩率3)<br>(磁盘400MB/s)如果是10B的列，就是1-2亿行/s。 </p>
<h2 id="功能多"><a href="#功能多" class="headerlink" title="功能多"></a>功能多</h2><p>最重要的是有<code>AggregatingMergeTree</code>表引擎，专门优化了三个数据分析最实用的查询:（海量数据快速计算）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uniq: 计算uv</span><br><span class="line">any: 抽样统计</span><br><span class="line">quantiles: 分位数</span><br></pre></td></tr></table></figure>
<p>上述几个功能如果用sparkSql/hive,一般耗时都是15分钟以上。(甚至到半小时、1小时)<br>如果用mysql的话,则由于维度爆炸的问题可能存不下这么多数据，并且无法灵活新增维度。<br>clickhouse对于海量数据处理没有spark/hive那么灵活,但是特化了OLAP的即时查询性能,本质上是处在不同领域的工具。<br>从数据仓库的角度来看:<br>ods层: 用spark/hive进行ETL后产生;<br>dw层: ods载入clickhouse后直接产生预聚合的数仓,支持即时查询;<br>dm层: mysql</p>
<h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>hbase/ES:一般用来支持海量数据点查询;<br>mysql: 用来支持无聚合的点查询;<br>clickhouse: 用来支持海量数据的聚合查询。<br>kylin: 比较接近clickhouse，底层是hbase+星型模型</p>
<p>其他引擎:</p>
<h4 id="ReplcingMergeTree"><a href="#ReplcingMergeTree" class="headerlink" title="ReplcingMergeTree"></a>ReplcingMergeTree</h4><p>删除相同主键的重复项(去重)</p>
<h3 id="SummingMergeTree"><a href="#SummingMergeTree" class="headerlink" title="SummingMergeTree"></a>SummingMergeTree</h3><p>将一个part中，相同主键的所有行聚合成一行，包含一系列聚合函数状态。</p>
<h3 id="CollapsingMergeTree"><a href="#CollapsingMergeTree" class="headerlink" title="CollapsingMergeTree"></a>CollapsingMergeTree</h3><p>提供折叠行功能： 把同主键的数据行去重到最多两行。（再次强调所有聚合都在part内）<br>场景: 用户访问状态记录、频繁变化的数据</p>
<p>前面说的clickhouse不支持update数据，所以用这个引擎可以近似达到一部分update的效果。<br>本质上就是类似于git的revert、银行系统里的冲正、mysql的MVCC。</p>
<p>比如我们要记录用户访问情况，先插入一条:<br><code>userid_0,5,146,1</code> 表示0号用户访问了5个页面，停留146秒(最后一列的1暂时忽略)。<br>然后过了一会儿想改成它访问了6个页面，停留185秒，那就插入:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">userid_0,5,146,-1</span><br><span class="line">userid_0,6,185,1</span><br></pre></td></tr></table></figure>
<p>首先把原来的取消掉，标记列-1。然后插入最新的状态数据，标记列1.</p>
<blockquote>
<p>应当注意这些成对的1,-1会异步地被删除，所以不能查到状态变化历史,仅用于查最新。</p>
</blockquote>
<p>这种引擎的建表语句:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> UAct</span><br><span class="line">(</span><br><span class="line">    UserID UInt64,</span><br><span class="line">    PageViews UInt8,</span><br><span class="line">    <span class="keyword">Duration</span> UInt8,</span><br><span class="line">    <span class="keyword">Sign</span> <span class="built_in">Int8</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ENGINE</span> = CollapsingMergeTree(<span class="keyword">Sign</span>)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> UserID</span><br></pre></td></tr></table></figure>
<p>查询的时候的语法:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    UserID,</span><br><span class="line">    <span class="keyword">sum</span>(PageViews * <span class="keyword">Sign</span>) <span class="keyword">AS</span> PageViews,</span><br><span class="line">    <span class="keyword">sum</span>(<span class="keyword">Duration</span> * <span class="keyword">Sign</span>) <span class="keyword">AS</span> <span class="keyword">Duration</span></span><br><span class="line"><span class="keyword">FROM</span> UAct</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> UserID</span><br><span class="line"><span class="keyword">HAVING</span> <span class="keyword">sum</span>(<span class="keyword">Sign</span>) &gt; <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p><code>CollapsingMergeTree</code>要求插入的顺序不能乱来，要按状态的变化顺序。<br>如果顺序无法保证，可以使用<code>VersionedCollapsingMergeTree</code>,它的算法也很简单,就是要求用户多传一个version字段。</p>
<h3 id="GraphiteMergeTree"><a href="#GraphiteMergeTree" class="headerlink" title="GraphiteMergeTree"></a>GraphiteMergeTree</h3><p>直接接到日志收集。<br>可以存metrics指标可视化系统Graphite的rollup数据。<br>如果不rollup，可以用别的引擎。</p>
<h3 id="Log系列的引擎-非主打"><a href="#Log系列的引擎-非主打" class="headerlink" title="Log系列的引擎(非主打)"></a>Log系列的引擎(非主打)</h3><p>用于小数据量(&lt; 100w)的表。<br>包括: StripeLog,Log,TinyLog三个引擎。</p>
<p>特性:</p>
<ul>
<li>追加写,不支持改</li>
<li>不支持索引</li>
<li>非原子写入(可能有损坏的数据)</li>
</ul>
<p>TinyLog：最简单的表引擎，适合一次写入即终身、多次查询的小数据，不支持并发数据访问，不支持同时写入同时读取。<br>Log：比TinyLog多一个偏移量优化.<br>Memory：以直接形式存储在内存中，读写变态快，但是记住是临时的，关机数据消失。<br>Buffer：缓冲，可以理解为将数据存储在内存中，然后达到一定阈值限制条件，那么先前的数据会自动写入设定的表格中。这样可以将部分热数据放在内存中缓存，快速访问和读取，而时间较为久远的数据写入表中释放内存，应该比较好理解。（可以实时盯数据）<br>External data：从字面理解，就是可以将文件数据等引入query语句中利用了。比如你想查找一些在你所给的名单中的用户的消费数据，那么你可以免除复制粘贴，直接将这个名单文件引入并使用，clickhouse会自动给这个文件建立一个临时表。</p>
<h3 id="其他功能"><a href="#其他功能" class="headerlink" title="其他功能:"></a>其他功能:</h3><ul>
<li>支持类SQL查询，相应的库函数很多：ip转换、数组、map、url转换、近似<br>计算uv、近似计算分位数、抽样统计等等；<br><a href="https://clickhouse.yandex/docs/zh/query_language/select/" target="_blank" rel="noopener">https://clickhouse.yandex/docs/zh/query_language/select/</a></li>
<li>数据源支持繁多，可以是以kafka、tcp、jdbc、文件等等直接作为表。</li>
<li>webUI支持: <a href="https://tabix.io/" target="_blank" rel="noopener">https://tabix.io/</a></li>
<li>IDE支持: jetbrain打造的datagrip插件: <a href="https://blog.jetbrains.com/datagrip/tag/clickhouse/" target="_blank" rel="noopener">https://blog.jetbrains.com/datagrip/tag/clickhouse/</a><br><a href="http://www.clickhouse.com.cn/topic/5b6ce6359d28dfde2ddc6229" target="_blank" rel="noopener">http://www.clickhouse.com.cn/topic/5b6ce6359d28dfde2ddc6229</a></li>
</ul>
<h2 id="可用性高"><a href="#可用性高" class="headerlink" title="可用性高"></a>可用性高</h2><p>任何时候随时可以给表添加字段、属性、维度，不会拖慢或影响集群运行速度。<br>BI系统很大的一个痛点是维度的<strong>组合爆炸</strong>，而且经常需要新增，clickhouse针对性地优化了这一点。(如果是mysql要新增维度列,需要重做整个表,即使是mysql8的瞬加字段也不行)</p>
<p>流水线式的数据处理流程，数据一旦进入系统，那么立即处于可以使用的状态，边读（查询）边写没有任何压力。</p>
<h1 id="How-clickhouse的底层实现原理"><a href="#How-clickhouse的底层实现原理" class="headerlink" title="How: clickhouse的底层实现原理"></a>How: clickhouse的底层实现原理</h1><p>主要思想是根据OLAP的特征舍弃了一部分功能，然后针对性地优化了一部分功能。主要方法包括LSM（MergeTree系列表引擎）、稀疏索引（缓存友好）、列式存储+数据压缩、VectorWise、用概率算法进行近似等等。</p>
<h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><h3 id="OLAP应用的特点"><a href="#OLAP应用的特点" class="headerlink" title="OLAP应用的特点:"></a>OLAP应用的特点:</h3><ol>
<li>大多数是读请求</li>
<li>数据总是以相当大的批(&gt; 1000 rows)进行写入</li>
<li>不修改已添加的数据</li>
<li>每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列</li>
<li>宽表，即每个表包含着大量的列</li>
<li>较少的查询(通常每台服务器每秒数百个查询或更少)</li>
<li>对于简单查询，允许延迟大约50毫秒</li>
<li>列中的数据相对较小： 数字和短字符串(例如，每个URL 60个字节)</li>
<li>处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行）</li>
<li>事务不是必须的</li>
<li>对数据一致性要求低</li>
<li>每一个查询除了一个大表外都很小</li>
<li>查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中</li>
</ol>
<h3 id="面临的困难："><a href="#面临的困难：" class="headerlink" title="面临的困难："></a>面临的困难：</h3><ol>
<li>维度组合爆炸；</li>
<li>聚合数据后,如果有修改很蛋疼.</li>
<li>URL这种无法预聚合.</li>
</ol>
<h3 id="需求洞察"><a href="#需求洞察" class="headerlink" title="需求洞察:"></a>需求洞察:</h3><p>用户只关心聚合后中极小一部分</p>
<p>市场上的备胎: sparkSQL,Impala,Drill都不好用。</p>
<h3 id="舍弃的功能"><a href="#舍弃的功能" class="headerlink" title="舍弃的功能"></a>舍弃的功能</h3><ol>
<li>事务支持</li>
<li>快速修改、删除数据。 (可以低速批量删除、修改)</li>
<li>点查询(检索单行): 因为用的是稀疏索引。<br>(好处是稀疏所以索引能完全放入内存，范围查询很快)</li>
<li>高并发查询: 只支持100/s量级查询,对于内网应用、分析型业务足够。 </li>
<li>窗口函数。</li>
</ol>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>基于上述几点需求分析的优化:</p>
<ol>
<li>cpu: VectorWise方法,将压缩的列数据整理成现代CPU容易处理的Vector模式。利用现代CPU的多线程。 SIMD: 每次处理一批Vector数据。</li>
<li>提高内存利用率: 稀疏索引;</li>
<li>硬盘: MergeTree系列表引擎(LSM算法),批量合并写入,提高IO吞吐率;</li>
<li>算法: 近似算法/概率算法。</li>
</ol>
<p>架构: 表=&gt;shard=&gt;replica=&gt;partiton=&gt;part</p>
<h2 id="稀疏索引"><a href="#稀疏索引" class="headerlink" title="稀疏索引"></a>稀疏索引</h2><p>对应<code>index_granularity</code>参数:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M(SettingUInt64, index_granularity, <span class="number">8192</span>, <span class="string">"How many rows correspond to one primary key value."</span>) \</span><br></pre></td></tr></table></figure>
<p>索引中相邻mark之间的数据行数,默认8192.<br>借助稀疏索引，它能存更多的索引在内存中。（相当于存了B树的前几层或二级索引）。</p>
<p>其他配置:<br><a href="https://github.com/yandex/ClickHouse/blob/master/dbms/src/Storages/MergeTree/MergeTreeSettings.h" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse/blob/master/dbms/src/Storages/MergeTree/MergeTreeSettings.h</a></p>
<p>比如io配置:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M(SettingUInt64, min_merge_bytes_to_use_direct_io, <span class="number">10U</span>LL * <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">1024</span>, <span class="string">"Minimal amount of bytes to enable O_DIRECT in merge (0 - disabled)."</span>) \</span><br></pre></td></tr></table></figure>
<p>超过多少Bytes以后绕过内核缓冲，进行直接IO。(节省内存开销、数据复制开销)</p>
<p>其他配置的分三大块:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Merge settings. */</span> \    合并时的配置</span><br><span class="line"><span class="comment">/** Inserts settings. */</span> \  插入时的配置</span><br><span class="line"><span class="comment">/** Replication settings. */</span> \ 副本的配置</span><br><span class="line"><span class="comment">/** Check delay of replicas settings. */</span> \ 副本检查延迟配置</span><br><span class="line"><span class="comment">/** Compatibility settings */</span> \   兼容性配置</span><br></pre></td></tr></table></figure>

<h3 id="稀疏索引示例"><a href="#稀疏索引示例" class="headerlink" title="稀疏索引示例"></a>稀疏索引示例</h3><p>数据存储:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">全部数据  :     [-------------------------------------------------------------------------]</span><br><span class="line">CounterID:      [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]</span><br><span class="line">Date:           [1111111222222233331233211111222222333211111112122222223111112223311122333]</span><br><span class="line">标记:            |      |      |      |      |      |      |      |      |      |      |</span><br><span class="line">                a,1    a,2    a,3    b,3    e,2    e,3    g,1    h,2    i,1    i,3    l,3</span><br><span class="line">标记号:          0      1      2      3      4      5      6      7      8      9      10</span><br></pre></td></tr></table></figure>
<ol>
<li>CounterID in (‘a’, ‘h’): [0, 3) 和 [6, 8) 区间</li>
<li>CounterID IN (‘a’, ‘h’) AND Date = 3 : [1, 3) 和 [7, 8) 区间</li>
<li>Date = 3: 扫全表。</li>
</ol>
<p>表由按主键排序的数据 <code>part</code> 组成。<br>当数据被插入到表中时，会分成<code>part</code>并按主键的字典序排序。例如，主键是 (CounterID, Date) 时，part中数据按 CounterID 排序，具有相同 CounterID 的部分按 Date 排序。</p>
<p>不会合并来自不同分区的数据片段。（性能考虑）<br>不保证相同主键的所有行都会合并到同一个数据片段中。(没有必要)</p>
<p>索引文件： 每个part创建一个<br>每隔index_granularity一个索引行号(mark)；<br>对于每列，跟主键相同的索引行处也会写入mark。这些mark让你可以直接找到数据所在的列。</p>
<h2 id="表引擎：MergeTree族引擎"><a href="#表引擎：MergeTree族引擎" class="headerlink" title="表引擎：MergeTree族引擎"></a>表引擎：MergeTree族引擎</h2><p>表引擎（即表的类型）决定了：</p>
<blockquote>
<p>数据的存储方式和位置，写到哪里以及从哪里读取数据<br>支持哪些查询以及如何支持。<br>并发数据访问。<br>索引的使用（如果存在）。<br>是否可以执行多线程请求。<br>数据复制参数。</p>
</blockquote>
<p>clickhouse中最强大的都是合并树引擎系列。</p>
<ul>
<li><p>理念:<br>批量写入,后台合并;</p>
</li>
<li><p>特点:</p>
</li>
</ul>
<ol>
<li>数据按主键排序; (类似于聚簇)</li>
<li>允许使用主键分区; (类似于Hive)</li>
<li>ReplicatedMergeTree系列支持副本(类似于hdfs)</li>
<li>支持数据采样;(类似于Mysql performanceSchema)</li>
</ol>
<p>建表语句:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr2],</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">INDEX</span> index_name1 expr1 <span class="keyword">TYPE</span> type1(...) GRANULARITY value1,</span><br><span class="line">    <span class="keyword">INDEX</span> index_name2 expr2 <span class="keyword">TYPE</span> type2(...) GRANULARITY value2</span><br><span class="line">) <span class="keyword">ENGINE</span> = MergeTree()</span><br><span class="line">[<span class="keyword">PARTITION</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[PRIMARY <span class="keyword">KEY</span> expr]</span><br><span class="line">[<span class="keyword">SAMPLE</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[<span class="keyword">SETTINGS</span> <span class="keyword">name</span>=<span class="keyword">value</span>, ...]</span><br></pre></td></tr></table></figure>
<p>示例语句:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ENGINE MergeTree() </span><br><span class="line">PARTITION BY toYYYYMM(EventDate) </span><br><span class="line">ORDER BY (CounterID, EventDate, intHash32(UserID)) </span><br><span class="line">SAMPLE BY intHash32(UserID) </span><br><span class="line">SETTINGS index_granularity=8192</span><br></pre></td></tr></table></figure>
<p>默认情况下主键跟排序键（由 ORDER BY 子句指定）相同。<br>这里可以看出它不支持唯一索引,重复是很自然的。由上层自己保证。</p>
<p>SummingMergeTree 和 AggregatingMergeTree 引擎中<br>列分为两种:</p>
<ul>
<li>维度</li>
<li>度量 (各种pv,uv等等)</li>
</ul>
<p>Mysql的做法是把所有维度作为主键; (每次新增维度很痛)<br>clickhouse的推荐做法是把旧的维度作为主键(保留少量),所有维度(旧维度+新维度)作为排序列。<br>这里排序列的修改是轻量级的:<br>旧的维度是整体排序列的前缀(已然有序)，仅需排序新加的行。</p>
<p>推荐使用方案:<br>原始数据=&gt; MergeTree （确保原始数据不丢失）<br>原始数据=&gt; SummingMergeTree/AggregatingMergeTree (得到预聚合数据)</p>
<blockquote>
<p>引擎会定期合并相同主键的数据进行聚合。最终结果中多半还是有重复主键，但是同一个part中不会有。</p>
</blockquote>
<p>具体来说：<br><code>SummingMergeTree</code>: 把相同排序列的行聚合。<br>被聚合的列在建表语句中通过<code>columns</code>指定。（数值、非主键）<br>(如果<code>columns</code>为空会聚合所有非排序列)</p>
<p>特殊情况:</p>
<ol>
<li>某行所有度量列值都是0，直接删除该行;(sum优化)</li>
<li>非数值(无法汇总): 随机选一个值.</li>
<li>支持sumMap函数: 某列是map结构。</li>
</ol>
<h4 id="AggregatingMergeTree引擎"><a href="#AggregatingMergeTree引擎" class="headerlink" title="AggregatingMergeTree引擎"></a>AggregatingMergeTree引擎</h4><p><code>SummingMergeTree</code>只支持算pv,<code>AggregatingMergeTree</code>能支持算uv,分位数,抽样,三个函数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uniq</span><br><span class="line">anyIf (any+If)</span><br><span class="line">quantiles</span><br></pre></td></tr></table></figure>

<p><strong>创建:</strong>(物化视图)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">MATERIALIZED</span> <span class="keyword">VIEW</span> test.basic</span><br><span class="line"><span class="keyword">ENGINE</span> = AggregatingMergeTree() <span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(StartDate) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (CounterID, StartDate)</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span></span><br><span class="line">    CounterID,</span><br><span class="line">    StartDate,</span><br><span class="line">    sumState(<span class="keyword">Sign</span>)    <span class="keyword">AS</span> Visits, <span class="comment">-- 聚合1: pv</span></span><br><span class="line">    uniqState(UserID) <span class="keyword">AS</span> <span class="keyword">Users</span>  <span class="comment">-- 聚合2: uv 注意是记录了状态(特定的二进制表示法)</span></span><br><span class="line"><span class="keyword">FROM</span> test.visits</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> CounterID, StartDate;</span><br></pre></td></tr></table></figure>
<p>插入数据的时候只需要插入到<code>test.visits</code>.<br>视图中也会有数据，并且会聚合。</p>
<p><strong>查询:</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    StartDate,</span><br><span class="line">    sumMerge(Visits) <span class="keyword">AS</span> Visits, <span class="comment">-- 注意都变成了merge后缀</span></span><br><span class="line">    uniqMerge(<span class="keyword">Users</span>) <span class="keyword">AS</span> <span class="keyword">Users</span></span><br><span class="line"><span class="keyword">FROM</span> test.basic</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> StartDate</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> StartDate;</span><br></pre></td></tr></table></figure>

<h2 id="算法-uniq"><a href="#算法-uniq" class="headerlink" title="算法: uniq"></a>算法: uniq</h2><p>上一节中<code>AggregatingMergeTree</code>的uniq求uv,其实有三个函数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uniq: 用UniquesHashSet近似求uv（BJKST算法）</span><br><span class="line">uniqHLL12: 用HLL近似求uv </span><br><span class="line">uniqExact: 用HashSet精确求uv</span><br></pre></td></tr></table></figure>

<p>源码见:<br><a href="https://github.com/yandex/ClickHouse/blob/master/dbms/src/AggregateFunctions/AggregateFunctionUniq.cpp" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse/blob/master/dbms/src/AggregateFunctions/AggregateFunctionUniq.cpp</a></p>
<p>其中HLL就是HyperLogLog算法。</p>
<p>而第一个<code>UniquesHashSet</code>(<a href="https://github.com/yandex/ClickHouse/blob/ef50601b5ceeeaf5763eab6c0013954c12eb00b1/dbms/src/AggregateFunctions/UniquesHashSet.h" target="_blank" rel="noopener">https://github.com/yandex/ClickHouse/blob/ef50601b5ceeeaf5763eab6c0013954c12eb00b1/dbms/src/AggregateFunctions/UniquesHashSet.h</a>)<br>两者的思想都是uv越大,不同的hash值越多。</p>
<p><code>UniquesHashSet</code>的特点是内存消耗小,性能高。<br>具体实现是将输入hash到UInt32,然后插入到数组中,如果遇到碰撞则进行线性探测. (原始输入丢弃,只存hash值)随着插入进行达到阈值<code>UNIQUES_HASH_MAX_SIZE</code>时，则将当前存的值丢弃一半,只保留能整除2的值,提高<code>skip_degree</code>值,然后开始只接受能整除2的输入。依此类推,后续就是只接受整除4，8，16的值。最后获取结果:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">size</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (<span class="number">0</span> == skip_degree)</span><br><span class="line">           <span class="keyword">return</span> m_size;</span><br><span class="line">       <span class="keyword">size_t</span> res = m_size * (<span class="number">1U</span>LL &lt;&lt; skip_degree);</span><br><span class="line">       <span class="comment">/** Pseudo-random remainder - in order to be not visible,</span></span><br><span class="line"><span class="comment">         * that the number is divided by the power of two.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">       res += (intHashCRC32(m_size) &amp; ((<span class="number">1U</span>LL &lt;&lt; skip_degree) - <span class="number">1</span>));</span><br><span class="line">       <span class="comment">/** Correction of a systematic error due to collisions during hashing in UInt32.</span></span><br><span class="line"><span class="comment">         * `fixed_res(res)` formula</span></span><br><span class="line"><span class="comment">         * - with how many different elements of fixed_res,</span></span><br><span class="line"><span class="comment">         *   when randomly scattered across 2^32 buckets,</span></span><br><span class="line"><span class="comment">         *   filled buckets with average of res is obtained.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">       <span class="keyword">size_t</span> p32 = <span class="number">1U</span>LL &lt;&lt; <span class="number">32</span>;</span><br><span class="line">       <span class="keyword">size_t</span> fixed_res = round(p32 * (<span class="built_in">log</span>(p32) - <span class="built_in">log</span>(p32 - res)));</span><br><span class="line">       <span class="keyword">return</span> fixed_res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>rehash的实现:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">rehash</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; buf_size(); ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (buf[i] &amp;&amp; !good(buf[i]))</span><br><span class="line">            &#123;</span><br><span class="line">                buf[i] = <span class="number">0</span>;</span><br><span class="line">                --m_size;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** After removing the elements, there may have been room for items,</span></span><br><span class="line"><span class="comment">          * which were placed further than necessary, due to a collision.</span></span><br><span class="line"><span class="comment">          * You need to move them.</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; buf_size(); ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (unlikely(buf[i] &amp;&amp; i != place(buf[i])))</span><br><span class="line">            &#123;</span><br><span class="line">                HashValue x = buf[i];</span><br><span class="line">                buf[i] = <span class="number">0</span>;</span><br><span class="line">                reinsertImpl(x);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>其中good函数含义就是能否被2^skip_degree整除。</p>
<ul>
<li>线性探测：<br>为了加快速度，增加了一个假设: 所有数据只插入Key/更新Key，不删除Key。<br>(这个假设在大数据处理/统计的场景下，大多都是成立的，spark中openHashSet也是线性探测)<br>有了这个假设它可以去掉拉链表，使用线性探测来实现哈希表。</li>
<li>内存利用率高: 去掉了8B指针结构，能够创建更大的哈希表，冲突减少；</li>
<li>内存紧凑: 位图操作快，一个内存page就能放下很多位图，8B就能放64个位置，缓存友好(while循环pos++)。</li>
</ul>
<h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><p>假如表结构是:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test.mergetree1 </span><br><span class="line">(sdt  <span class="built_in">Date</span></span><br><span class="line">, <span class="keyword">id</span> UInt16</span><br><span class="line">, <span class="keyword">name</span> <span class="keyword">String</span></span><br><span class="line">, cnt UInt16) </span><br><span class="line"><span class="keyword">ENGINE</span>=MergeTree(sdt, (<span class="keyword">id</span>, <span class="keyword">name</span>), <span class="number">10</span>);</span><br></pre></td></tr></table></figure>
<p>分区字段是日期sdt.<br>对应的目录结构:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">├── 20180601_20180601_1_1_0</span><br><span class="line">│   ├── checksums.txt</span><br><span class="line">│   ├── columns.txt <span class="comment">-- 元数据</span></span><br><span class="line">│   ├── id.bin <span class="comment">-- 压缩列</span></span><br><span class="line">│   ├── id.mrk <span class="comment">-- 索引mark</span></span><br><span class="line">│   ├── name.bin</span><br><span class="line">│   ├── name.mrk</span><br><span class="line">│   ├── cnt.bin</span><br><span class="line">│   ├── cnt.mrk </span><br><span class="line">│   ├── cnt.idx</span><br><span class="line">│   ├── primary.idx <span class="comment">-- 主键</span></span><br><span class="line">│   ├── sdt.bin</span><br><span class="line">│   └── sdt.mrk <span class="comment">-- 保存一下块偏移量</span></span><br><span class="line">├── 20180602_20180602_2_2_0</span><br><span class="line">│   └── ...</span><br><span class="line">├── 20180603_20180603_3_3_0</span><br><span class="line">│   └── ...</span><br><span class="line">├── format_version.txt</span><br><span class="line">└── detached <span class="comment">-- 破损数据</span></span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>clickhouse为啥比hive/spark快:</p>
<ul>
<li>7*24小时都在后台预聚合.hive/spark计算的时候才申请资源,平时只占一点点;</li>
<li>可以用星型模型缩减数据类型、压缩友好;</li>
<li>计算过程没有hive/spark中的shuffle概念,全是mapAgg;</li>
</ul>
<p>clickhouse为啥比mysql快:(仅限clickhouse擅长的查询)</p>
<ul>
<li>预聚合</li>
<li>多核优化、vector优化更彻底</li>
<li>分区+稀疏索引,整个索引能放内存,然后并发查part(这点还是要结合多核优化)</li>
<li>根据排序键排序存放 </li>
</ul>
<p>优化的方面:</p>
<ol>
<li>cpu: VectorWise方法,将压缩的列数据整理成现代CPU容易处理的Vector模式。利用现代CPU的多线程。 SIMD: 每次处理一批Vector数据。</li>
<li>提高内存利用率: 稀疏索引;</li>
<li>硬盘: MergeTree系列表引擎(LSM算法),批量合并写入,提高IO吞吐率,牺牲随机读能力;</li>
<li>算法: 近似算法/概率算法,HLL\BJKST算法等。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/07/06/2019-07/%E7%B2%97%E4%BF%97%E7%90%86%E8%A7%A3clickhouse/" data-id="ck96cxppz00j2maamamimdlei" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/clickhouse/" rel="tag">clickhouse</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-06/io中的缓冲" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/22/2019-06/io%E4%B8%AD%E7%9A%84%E7%BC%93%E5%86%B2/" class="article-date">
  <time datetime="2019-06-22T02:56:52.000Z" itemprop="datePublished">2019-06-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/java/">java</a>►<a class="article-category-link" href="/categories/java/Netty/">Netty</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/22/2019-06/io%E4%B8%AD%E7%9A%84%E7%BC%93%E5%86%B2/">io中的缓冲——如何理解O_Direct</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>IO缓冲主要有4层:<br>1.用户自己的缓冲;<br>2.库缓冲;<br>3.内核缓冲;<br>4.磁盘缓冲。</p>
<img src="/images/2019-06/io_layer.gif" class="" width="400" height="600" title="io_layer">
<p>=== 应用层: (进程挂丢数据) 看到文件句柄<br><code>application buffer</code>: 比如我们代码中写的int[]arrayData;<br><code>clib buffer</code>: <code>fwrite</code>以后到这层。这里写的是c库(<code>IObuffer</code>)，也可能是java库中的缓冲(<code>BufferedOutputStream</code>)。<br>如果数据才到这一层库缓冲，还没系统调用，此时程序core dump的话，数据就丢了。<br>=== 内核层: (内核挂丢数据)  看到inode和数据块<br><code>page cache</code>: 内核层的缓冲。fflush以后到这里, fclose先到这里然后继续到磁盘。<br><code>driver</code>: 具体设备的驱动软件<br>=== 设备层: （断电丢数据） 看到扇区<br><code>disk cache</code>: 磁盘缓冲。fsync/fclose至少到这里。fsync是同步会完全等返回。</p>
<h2 id="为啥要有库缓冲"><a href="#为啥要有库缓冲" class="headerlink" title="为啥要有库缓冲"></a>为啥要有库缓冲</h2><p>（比如<code>clib buffer</code>）<br>因为从应用层到内核层需要系统调用、内核态切换，开销比较大，为了减少这件事发生的次数，没有必要因为1个字节的改动发生系统调用。</p>
<h3 id="绕过库缓冲的方案"><a href="#绕过库缓冲的方案" class="headerlink" title="绕过库缓冲的方案:"></a>绕过库缓冲的方案:</h3><p>用mmap(内存映射文件),把内核空间的page cache映射到用户空间。</p>
<h2 id="为啥要有内核缓冲"><a href="#为啥要有内核缓冲" class="headerlink" title="为啥要有内核缓冲"></a>为啥要有内核缓冲</h2><p>内核用pdflush线程循环检测脏页,判断是否写回磁盘。<br>由于磁盘是单向旋转,重新排布写操作顺序可以减少旋转次数。(合并写入)<br>plus:<br><code>O_SYNC</code>参数: 访问内核缓冲时是异步还是同步。<code>O_SYNC</code>表示同步。</p>
<h3 id="绕过内核缓冲的方案"><a href="#绕过内核缓冲的方案" class="headerlink" title="绕过内核缓冲的方案"></a>绕过内核缓冲的方案</h3><p>用<code>O_Direct</code>参数，直接怼Disk cache。</p>
<h2 id="为啥要有磁盘缓冲"><a href="#为啥要有磁盘缓冲" class="headerlink" title="为啥要有磁盘缓冲"></a>为啥要有磁盘缓冲</h2><p>驱动通过DMA，将数据写入磁盘cache。<br>磁盘缓冲主要是保护磁盘不被cpu写坏。是与外部总线交换数据的场所。（断电丢数据）</p>
<h4 id="绕过磁盘缓冲的方案"><a href="#绕过磁盘缓冲的方案" class="headerlink" title="绕过磁盘缓冲的方案"></a>绕过磁盘缓冲的方案</h4><p>用<code>RAW设备写</code>,直接写扇区: fdisk,dd,cpio工具。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/06/22/2019-06/io%E4%B8%AD%E7%9A%84%E7%BC%93%E5%86%B2/" data-id="ck96cxppt00inmaam0iy74lrn" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/io/" rel="tag">io</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-06/领域驱动设计-第一-六章笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/08/2019-06/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1-%E7%AC%AC%E4%B8%80-%E5%85%AD%E7%AB%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2019-06-08T02:25:04.000Z" itemprop="datePublished">2019-06-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/java/">java</a>►<a class="article-category-link" href="/categories/java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/08/2019-06/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1-%E7%AC%AC%E4%B8%80-%E5%85%AD%E7%AB%A0%E7%AC%94%E8%AE%B0/">领域驱动设计-第一~六章笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>领域驱动设计(DDD)这本书主要是讲抽象概念、理念、思想，具体可以有不同的实现，具体明确一些尺度或者细节。</p>
<h1 id="第一章-消化知识"><a href="#第一章-消化知识" class="headerlink" title="第一章 消化知识"></a>第一章 消化知识</h1><p>这一节主要讲了作者开发一个用于印刷电路板(PCB)的软件工具的过程。<br>他请教了相关专家，逐步建模。</p>
<h2 id="涉及到的角色："><a href="#涉及到的角色：" class="headerlink" title="涉及到的角色："></a>涉及到的角色：</h2><p>软件开发工程师、业务方</p>
<h2 id="涉及到的过程："><a href="#涉及到的过程：" class="headerlink" title="涉及到的过程："></a>涉及到的过程：</h2><p>需求分析、提炼模型<br>消化PCB相关的业务知识、<br>形成业务方和开发工程师都能看懂的共同语言/名词（类似于DSL）</p>
<p>我的理解是相当于产品做需求分析的那步，由程序员直接参与，快速迭代讨论形成DSL，省得吃产品理解的二手需求经常出错。</p>
<h2 id="DSL相关"><a href="#DSL相关" class="headerlink" title="DSL相关:"></a>DSL相关:</h2><p>1.4节还提到用策略设计模式来写代码，让需求方也能看懂代码中运用的策略（可读性），让DSL范围扩散到架构图和顶层代码。</p>
<h1 id="第二章-语言的使用"><a href="#第二章-语言的使用" class="headerlink" title="第二章 语言的使用"></a>第二章 语言的使用</h1><p>按我理解还是要形成DSL，图简略。<br>语言精简程度=&gt;大量使用短语。</p>
<h1 id="第三章-绑定模型和实现"><a href="#第三章-绑定模型和实现" class="headerlink" title="第三章 绑定模型和实现"></a>第三章 绑定模型和实现</h1><p>模型和实现要对应，不然就难以维护实现，模型失去意义。<br>推荐了一下面向对象编程。<br>建模与代码实现不能完全分离。</p>
<h1 id="第二部分-模型驱动设计的构造块"><a href="#第二部分-模型驱动设计的构造块" class="headerlink" title="第二部分 模型驱动设计的构造块"></a>第二部分 模型驱动设计的构造块</h1><p>设计原则：职责驱动<br>（SOLID原则）</p>
<img src="/images/2019-06/vo.png" class="" width="800" height="1200" title="vo">

<p>模型驱动涉及到的几个构造块的概念：<br><code>Service</code>: 服务;<br><code>Entity</code>: 实体,可以理解成数据库中的一张表;<br><code>Value Object</code>: 值对象,传输中的不可变对象,为了明确不可变的特性;<br><code>Factory</code>: 工厂;<br><code>Reposity</code>: 负责隐藏存储层细节;<br><code>Aggregate</code>: 负责封装多组VO/实体，聚合;(有说法这个不是某个类，而是一个虚拟概念)<br><code>Aggregate Root</code>: 一个实体，作为网关服务，修改生成Aggregate，服务给外界.（我理解就是粗粒度封装entity,减少整个领域的接口数量、暴露在外的引用数量）</p>
<p>领域外的术语:<br><code>DTO</code>: 和VO太像了; 但不在领域驱动的语境里；<br><code>POJO</code>: 纯粹属于语法语境了;</p>
<h1 id="第四章-分离领域"><a href="#第四章-分离领域" class="headerlink" title="第四章 分离领域"></a>第四章 分离领域</h1><img src="/images/2019-06/ddd_layer.png" class="" width="800" height="1200" title="ddd_layer">

<p>分离领域，也就是分层架构，然后重点是把领域层分离出来。<br>ddd的四个概念层:</p>
<ul>
<li>UI层/表示层: view</li>
<li>应用层: 尽量薄,按我理解就是controller</li>
<li>领域层/模型层: 业务概念、状态、规则（按我理解就是logic）</li>
<li>基础设施层：持久化、消息传递、UI渲染</li>
</ul>
<p>层之间松散连接、单向依赖。<br>如果下层要调用上层: 回调、观察者模式。</p>
<p>不要把所有东西都放到<code>applicationContext</code>里头，只放助于解耦的部分。<br>这个是出于性能考虑，毕竟spring是用一个<code>concurrentHashMap</code>作为Ioc容器的。啥都往里头奔放了。<br>(可以只放大粒度对象。</p>
<p>smartUI: 就是在UI代码里写很多逻辑,巨复杂不好维护。不推荐。</p>
<h1 id="第五章-软件中所表示的模型"><a href="#第五章-软件中所表示的模型" class="headerlink" title="第五章 软件中所表示的模型"></a>第五章 软件中所表示的模型</h1><p><code>Entity</code>: 实体,可以跨实现跟踪,(可以在数据库中查到).<br><code>Value Object</code>: <code>VO</code>, 仅用于传输，是某个状态的镜像，不可变。<br><code>Service</code>: 服务，只封装方法,无状态，可以放心调用。粒度中等为好。</p>
<h2 id="简化关系"><a href="#简化关系" class="headerlink" title="简化关系"></a>简化关系</h2><p>遇到多对多、双向关联关系，可以寻找自然偏向，从而把它简化为：1对多，单向关联。</p>
<p>例子:<br>一般问美国1790年的总统是谁，而比较少问<code>华盛顿</code>是哪个国家的总统。<br>所以国家和总统的关系可以简化为单向。</p>
<h1 id="第六章-领域对象的生命周期"><a href="#第六章-领域对象的生命周期" class="headerlink" title="第六章 领域对象的生命周期"></a>第六章 领域对象的生命周期</h1><h2 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a>Aggregate</h2><p> 根/<code>Aggregate Root</code>: 汽车Entity<br> 边界内实体/<code>Entity</code>: 轮胎</p>
<p> <code>Aggregate</code>外部只引用根。(利于垃圾收集)<br> 边界内其他实体以<code>Value Object</code>形式交付给外部。(不可变，利于修改可控,修改由根控制，保证一致性)</p>
<h2 id="Factory"><a href="#Factory" class="headerlink" title="Factory"></a>Factory</h2><p> 创建对象或Aggregate工作很复杂的时候，可以引入Factory来封装。<br> Factory的设计模式包括:</p>
<ul>
<li><p>简单工厂: 对字符串switch case;或传入类名,反射创建对象;</p>
</li>
<li><p>工厂: 不把所有创建放一个类,每个类有自己的工厂,巨啰嗦;(符合开闭原则,用冗余来获取简单)</p>
</li>
<li><p>抽象工厂</p>
</li>
<li><p>Builder模式</p>
<p>Factory的作用:</p>
<ol>
<li>把和运行时工作无关的复杂创建逻辑抽离到别的地方；</li>
<li>解耦两个类的关联：(跨Agg边界的两个类) 比如需要用账号类创建交易对象，但是本质上账号对象和交易对象关系比较弱，可以用工厂来创建，这样耦合度低一些。</li>
</ol>
</li>
</ul>
<p>防止Factory的滥用导致类膨胀，正常情况直接用构造函数。<br>Factory还可以负责反序列化，重建对象。</p>
<h2 id="Repository"><a href="#Repository" class="headerlink" title="Repository"></a>Repository</h2><p>封装对数据的访问,方便随时切换底层实现(内存哑实现、缓存)</p>
<h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p><code>repository</code>不封装事务，事务的决定权留给调用方。</p>
<h2 id="Factory-vs-Repository"><a href="#Factory-vs-Repository" class="headerlink" title="Factory vs Repository"></a>Factory vs Repository</h2><p><code>Factory</code>: 生命周期的开始(Entity的创建);<br><code>Repository</code>: 生命周期的中间和结束(从数据库查出数据重建Entity不算新建).</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/06/08/2019-06/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1-%E7%AC%AC%E4%B8%80-%E5%85%AD%E7%AB%A0%E7%AC%94%E8%AE%B0/" data-id="ck96cxppw00iumaam2vgl7oiv" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DDD/" rel="tag">DDD</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1/" rel="tag">设计</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-05/spark-sql中的分位数算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/26/2019-05/spark-sql%E4%B8%AD%E7%9A%84%E5%88%86%E4%BD%8D%E6%95%B0%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2019-05-26T12:36:51.000Z" itemprop="datePublished">2019-05-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/26/2019-05/spark-sql%E4%B8%AD%E7%9A%84%E5%88%86%E4%BD%8D%E6%95%B0%E7%AE%97%E6%B3%95/">spark-sql中的分位数算法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>spark有两个分位数算法:</p>
<ol>
<li><code>percentile</code>: 接受Int,Long,精确计算。底层用OpenHashMap,计数，然后排序key;</li>
<li><code>percentile_approx</code>：接受Int,Long,Double,近似计算。用的GK算法。论文参见《Space-efficient Online Computation of Quantile Summaries》(<a href="http://dx.doi.org/10.1145/375663.375670" target="_blank" rel="noopener">http://dx.doi.org/10.1145/375663.375670</a>)<br>基本思想是以最小空间完成分位数统计，比如把相邻的1w个数压缩成(平均数,个数)元组。如果空间够用，就不进行这种压缩。（所以如果如果统计90分位数，传入的精度参数至少应为10，如果统计999分位数，传入的精度参数至少为1000，默认精度是10000。）</li>
</ol>
<p>俩算法和Hive版本的基本是一样的。<br>区别: </p>
<ol>
<li>spark的percentile多了一个频次参数,也就是可以接受分阶段聚合；(percentile_approx木有)</li>
<li>spark底层用的openHashMap,速度快5倍,内存消耗更少。</li>
</ol>
<h2 id="为啥OpenHashMap性能优于HashMap"><a href="#为啥OpenHashMap性能优于HashMap" class="headerlink" title="为啥OpenHashMap性能优于HashMap?"></a>为啥OpenHashMap性能优于HashMap?</h2><p><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala</a><br><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala</a><br>OpenHashMap为了加快速度，增加了一个假设: 所有数据只插入Key/更新Key，不删除Key。<br>(这个假设在大数据处理/统计的场景下，大多都是成立的)<br>有了这个假设它可以去掉拉链表，使用线性探测的开放定址法来实现哈希表。</p>
<p>OpenHashMap底层数据委托给了OpenHashSet，所以本质上是看OpenHashSet为啥快。<br><code>OpenHashSet</code>用BitSet(位图)来存储在不在集合中(位运算，很快)，另开一个数组存储实际数据：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">var</span> _bitset = <span class="keyword">new</span> <span class="type">BitSet</span>(_capacity)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">var</span> _data: <span class="type">Array</span>[<span class="type">T</span>] = _</span><br><span class="line">  _data = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](_capacity)</span><br></pre></td></tr></table></figure>
<p>这俩成员始终保持等长，_bitset的下标x位置为1时，_data的下标x位置为中就有实际数据。(手动维持联动)<br>插入数据时，hash一下key生成pos，看看_bitset中对应位置有没有被占用，有的话就死循环++pos：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addWithoutResize</span></span>(k: <span class="type">T</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> pos = hashcode(hasher.hash(k)) &amp; _mask</span><br><span class="line">    <span class="keyword">var</span> delta = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!_bitset.get(pos)) &#123;</span><br><span class="line">        <span class="comment">// This is a new key.</span></span><br><span class="line">        _data(pos) = k</span><br><span class="line">        _bitset.set(pos)</span><br><span class="line">        _size += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> pos | <span class="type">NONEXISTENCE_MASK</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (_data(pos) == k) &#123;</span><br><span class="line">        <span class="comment">// Found an existing key.</span></span><br><span class="line">        <span class="keyword">return</span> pos</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// quadratic probing with values increase by 1, 2, 3, ...</span></span><br><span class="line">        pos = (pos + delta) &amp; _mask</span><br><span class="line">        delta += <span class="number">1</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Should never reach here."</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>逻辑很简单，由于假设了不会删除key,线性探测法变得实用。</p>
<h3 id="小结一下OpenHashSet快的原因"><a href="#小结一下OpenHashSet快的原因" class="headerlink" title="小结一下OpenHashSet快的原因:"></a>小结一下OpenHashSet快的原因:</h3><ol>
<li>内存利用率高: 去掉了8B指针结构，能够创建更大的哈希表，冲突减少；</li>
<li>内存紧凑: 位图操作快，一个内存page就能放下很多位图，8B就能放64个位置，缓存友好(while循环pos++)。</li>
</ol>
<h1 id="percentile实现"><a href="#percentile实现" class="headerlink" title="percentile实现:"></a>percentile实现:</h1><p><code>Percentile.scala</code>文件:<br><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Percentile.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Percentile.scala</a><br>首先看注释:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;* Because the number of elements and their partial order cannot be determined in advance.</span><br><span class="line"> * Therefore we have to store all the elements in memory, and so notice that too many elements can</span><br><span class="line"> * cause GC paused and eventually OutOfMemory Errors.</span><br><span class="line">&#x2F;</span><br></pre></td></tr></table></figure>
<p>基本思想是把所有元素保存在内存中。<br>因此它其实支持两阶段聚合:<br><code>_FUNC_(col, percentage [, frequency])</code><br>可以传入一个参数frequency表示频次.<br>// 2017-02-07加上的特性，比我写hive版本的分阶段聚合udaf早了10个月。</p>
<h1 id="percentile-approx实现"><a href="#percentile-approx实现" class="headerlink" title="percentile_approx实现"></a>percentile_approx实现</h1><p>代码:<br><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentile.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentile.scala</a><br><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/QuantileSummaries.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/QuantileSummaries.scala</a><br>底层委托给<code>QuantileSummaries</code>实现的。<br>主要有俩个成员变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample: Array[Stat] : 存放桶，超过1000个桶的时候就压缩（生成新的三元组）；</span><br><span class="line">headSampled: ArrayBuffer[Double]：缓冲区，每次达到5000个，就排序后更新到sample.</span><br></pre></td></tr></table></figure>
<p>主要思想是减少空间占用，因此很多排序，spark的实现merge sample的时候甚至都没有管俩sample已经有序了，直接sort了：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// <span class="doctag">TODO:</span> could replace full sort by ordered merge, the two lists are known to be sorted already.</span></span><br><span class="line"> <span class="keyword">val</span> res = (sampled ++ other.sampled).sortBy(_.value)</span><br><span class="line">  <span class="keyword">val</span> comp = compressImmut(res, mergeThreshold = <span class="number">2</span> * relativeError * count)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">QuantileSummaries</span>(</span><br><span class="line">    other.compressThreshold, other.relativeError, comp, other.count + count)</span><br></pre></td></tr></table></figure>
<p>Stat的定义:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Statistics from the Greenwald-Khanna paper.</span></span><br><span class="line"><span class="comment">  * @param value the sampled value</span></span><br><span class="line"><span class="comment">  * @param g the minimum rank jump from the previous value's minimum rank</span></span><br><span class="line"><span class="comment">  * @param delta the maximum span of the rank.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Stats</span>(<span class="params">value: <span class="type">Double</span>, g: <span class="type">Int</span>, delta: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>插入的函数:(每N个数，排序至少1次(merge还有1次)，因此是O(NlogN))</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(x: <span class="type">Double</span>): <span class="type">QuantileSummaries</span> = &#123;</span><br><span class="line">    headSampled += x</span><br><span class="line">    <span class="keyword">if</span> (headSampled.size &gt;= defaultHeadSize) &#123;</span><br><span class="line">      <span class="keyword">val</span> result = <span class="keyword">this</span>.withHeadBufferInserted</span><br><span class="line">      <span class="keyword">if</span> (result.sampled.length &gt;= compressThreshold) &#123;</span><br><span class="line">        result.compress()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        result</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">this</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>插入数据的其中一个步骤:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withHeadBufferInserted</span></span>: <span class="type">QuantileSummaries</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (headSampled.isEmpty) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">var</span> currentCount = count</span><br><span class="line">    <span class="keyword">val</span> sorted = headSampled.toArray.sorted</span><br><span class="line">    <span class="keyword">val</span> newSamples: <span class="type">ArrayBuffer</span>[<span class="type">Stats</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Stats</span>]()</span><br><span class="line">    <span class="comment">// The index of the next element to insert</span></span><br><span class="line">    <span class="keyword">var</span> sampleIdx = <span class="number">0</span></span><br><span class="line">    <span class="comment">// The index of the sample currently being inserted.</span></span><br><span class="line">    <span class="keyword">var</span> opsIdx: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (opsIdx &lt; sorted.length) &#123;</span><br><span class="line">      <span class="keyword">val</span> currentSample = sorted(opsIdx)</span><br><span class="line">      <span class="comment">// Add all the samples before the next observation.</span></span><br><span class="line">      <span class="keyword">while</span> (sampleIdx &lt; sampled.length &amp;&amp; sampled(sampleIdx).value &lt;= currentSample) &#123;</span><br><span class="line">        newSamples += sampled(sampleIdx)</span><br><span class="line">        sampleIdx += <span class="number">1</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If it is the first one to insert, of if it is the last one</span></span><br><span class="line">      currentCount += <span class="number">1</span></span><br><span class="line">      <span class="keyword">val</span> delta =</span><br><span class="line">        <span class="keyword">if</span> (newSamples.isEmpty || (sampleIdx == sampled.length &amp;&amp; opsIdx == sorted.length - <span class="number">1</span>)) &#123;</span><br><span class="line">          <span class="number">0</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          math.floor(<span class="number">2</span> * relativeError * currentCount).toInt</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> tuple = <span class="type">Stats</span>(currentSample, <span class="number">1</span>, delta)</span><br><span class="line">      newSamples += tuple</span><br><span class="line">      opsIdx += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add all the remaining existing samples</span></span><br><span class="line">    <span class="keyword">while</span> (sampleIdx &lt; sampled.length) &#123;</span><br><span class="line">      newSamples += sampled(sampleIdx)</span><br><span class="line">      sampleIdx += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">QuantileSummaries</span>(compressThreshold, relativeError, newSamples.toArray, currentCount)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>获取结果:O(n)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Target rank</span></span><br><span class="line">    <span class="keyword">val</span> rank = math.ceil(quantile * count).toInt</span><br><span class="line">    <span class="keyword">val</span> targetError = math.ceil(relativeError * count)</span><br><span class="line">    <span class="comment">// Minimum rank at current sample</span></span><br><span class="line">    <span class="keyword">var</span> minRank = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; sampled.length - <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> curSample = sampled(i)</span><br><span class="line">      minRank += curSample.g</span><br><span class="line">      <span class="keyword">val</span> maxRank = minRank + curSample.delta</span><br><span class="line">      <span class="keyword">if</span> (maxRank - targetError &lt;= rank &amp;&amp; rank &lt;= minRank + targetError) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="type">Some</span>(curSample.value)</span><br><span class="line">      &#125;</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h1 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h1><p>结合yuange在微博/km上分享的思路，用计数器区代替密集数据区的hashmap(其实也是GK算法的精确版)。逼近O(N)复杂度。<br>// TODO benchmark、优化算法</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/05/26/2019-05/spark-sql%E4%B8%AD%E7%9A%84%E5%88%86%E4%BD%8D%E6%95%B0%E7%AE%97%E6%B3%95/" data-id="ck96cxppq00icmaamdg6zaf5x" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-05/spark中编写UDAF的4种姿势" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/11/2019-05/spark%E4%B8%AD%E7%BC%96%E5%86%99UDAF%E7%9A%844%E7%A7%8D%E5%A7%BF%E5%8A%BF/" class="article-date">
  <time datetime="2019-05-11T12:15:05.000Z" itemprop="datePublished">2019-05-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/11/2019-05/spark%E4%B8%AD%E7%BC%96%E5%86%99UDAF%E7%9A%844%E7%A7%8D%E5%A7%BF%E5%8A%BF/">spark中编写UDAF的4种姿势</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>摘要: </p>
<blockquote>
<p>(探索解决sql的多行处理能力盲区)</p>
</blockquote>
<ol>
<li><p>搭配collect_set+UDF;</p>
</li>
<li><p>RDD: combineByKey;</p>
</li>
<li><p>Dataframe: 继承UserDefinedAggregateFunction;</p>
</li>
<li><p>Dataset: 继承Aggregator。</p>
<p>前文探索了解决sql对于单行处理的能力盲区(<a href="http://xiaoyue26.github.io/2019/05/08/2019-05/%E5%B0%86pyspark%E4%B8%AD%E7%9A%84UDF%E5%8A%A0%E9%80%9F50/">http://xiaoyue26.github.io/2019/05/08/2019-05/%E5%B0%86pyspark%E4%B8%AD%E7%9A%84UDF%E5%8A%A0%E9%80%9F50/</a> )，本文接着探索解决sql对于多行处理(UDAF/用户自定义聚合函数)的能力盲区。</p>
</li>
</ol>
<h1 id="姿势1：搭配collect-set-UDF"><a href="#姿势1：搭配collect-set-UDF" class="headerlink" title="姿势1：搭配collect_set+UDF"></a>姿势1：搭配collect_set+UDF</h1><p>基本思想是强行把一个group行拼成一个数组，然后编写一个能处理数组的UDF即可。如果是多行变多行，则UDF的输出也构造成数组，然后用explode打开。如果想要把多行聚合成一行（类似于sum），则直接输出结果即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str_list2idfa</span><span class="params">(txt_list)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = list()</span><br><span class="line">        <span class="keyword">for</span> txt <span class="keyword">in</span> txt_list:</span><br><span class="line">            res.append(str2idfa(txt))</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(app_name).getOrCreate()</span><br><span class="line">    provider = TDWSQLProvider(spark, user=user, passwd=passwd, db=db_name)</span><br><span class="line"></span><br><span class="line">    in_df = provider.table(in_table_name, [<span class="string">'p_2019042100'</span>])  <span class="comment"># 分区数组</span></span><br><span class="line">    print(in_df.columns)</span><br><span class="line">    <span class="comment"># 1. 创建udaf:</span></span><br><span class="line">    str_list2idfa_udaf = udf(str_list2idfa  <span class="comment"># 实际运行的函数</span></span><br><span class="line">                            , ArrayType(ArrayType(StringType()))  <span class="comment"># 函数返回值类型</span></span><br><span class="line">                            )</span><br><span class="line">    <span class="comment"># 2. 在df中使用,将数组转成二维数组:</span></span><br><span class="line">    out_t1 = in_df.groupBy(<span class="string">'uin'</span>).agg(</span><br><span class="line">        str_list2idfa_udaf(</span><br><span class="line">            collect_set(<span class="string">'value'</span>)</span><br><span class="line">        ).alias(<span class="string">'value_list'</span>)</span><br><span class="line">    )</span><br><span class="line">    print(out_t1.columns)</span><br><span class="line">    out_t1.printSchema()</span><br><span class="line">    out_t1.createOrReplaceTempView(<span class="string">"t1"</span>)</span><br><span class="line">    <span class="comment"># 3. 将二维数组打开,一行变多行,一列变两列:</span></span><br><span class="line">    out_t2 = spark.sql(<span class="string">'''</span></span><br><span class="line"><span class="string">    select uin</span></span><br><span class="line"><span class="string">          ,idfa_idfv[0] as idfa</span></span><br><span class="line"><span class="string">          ,idfa_idfv[1] as idfv</span></span><br><span class="line"><span class="string">    from t1</span></span><br><span class="line"><span class="string">    lateral view explode(value_list) tt as idfa_idfv</span></span><br><span class="line"><span class="string">    '''</span>)</span><br><span class="line">    out_t2.printSchema()</span><br><span class="line">    print(out_t2.take(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>优点： 开发成本低，不用编译。</li>
<li>缺点： 性能一般，增加了转换数组、explode的成本，可能导致聚合过程完全在单点进行，对于数据倾斜的承受能力较低。</li>
</ul>
<h1 id="姿势2：-使用RDD的combineByKey算子"><a href="#姿势2：-使用RDD的combineByKey算子" class="headerlink" title="姿势2： 使用RDD的combineByKey算子"></a>姿势2： 使用RDD的combineByKey算子</h1><p>  上述方法本质上是用UDF强行模拟了UDAF的功能，性能上有所损失。第二种方法是使用RDD的<code>combineByKey</code>算子: </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.appName(<span class="string">"UdafDemo"</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> rddProvider = <span class="keyword">new</span> <span class="type">TDWProvider</span>(spark.sparkContext, user, pass, db) <span class="comment">// 这个返回rdd</span></span><br><span class="line"><span class="keyword">val</span> inRdd = rddProvider.table(<span class="string">"t_dw_dc0xxxx"</span>, <span class="type">Array</span>(<span class="string">"p_2019042100"</span>))</span><br><span class="line">println(<span class="string">"getNumPartitions:"</span>)</span><br><span class="line">println(inRdd.getNumPartitions)</span><br><span class="line"><span class="keyword">val</span> kvRdd: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = inRdd</span><br><span class="line">  .map(row =&gt; (row(<span class="number">3</span>).toLong, <span class="type">UdfUtils</span>.str2idfa(row(<span class="number">9</span>))))</span><br><span class="line">  .filter(x =&gt; x._2.isDefined)</span><br><span class="line">  .map(x =&gt; (x._1, x._2.get))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> combineRdd: <span class="type">RDD</span>[(<span class="type">Long</span>, mutable.<span class="type">Set</span>[<span class="type">String</span>])] = kvRdd</span><br><span class="line">  .combineByKey(</span><br><span class="line">    (v: <span class="type">String</span>) =&gt; mutable.<span class="type">Set</span>(v),</span><br><span class="line">    (set: mutable.<span class="type">Set</span>[<span class="type">String</span>], v: <span class="type">String</span>) =&gt; set += v,</span><br><span class="line">    (set1: mutable.<span class="type">Set</span>[<span class="type">String</span>], set2: mutable.<span class="type">Set</span>[<span class="type">String</span>]) =&gt; set1 ++= set2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> outRdd: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = combineRdd.flatMap(kv =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> uin = kv._1</span><br><span class="line">  <span class="keyword">val</span> set = kv._2</span><br><span class="line">  <span class="keyword">val</span> res = mutable.<span class="type">MutableList</span>.empty[(<span class="type">Long</span>, <span class="type">String</span>)]</span><br><span class="line">  set.foreach(x =&gt; res += ((uin, x)))</span><br><span class="line">  res.iterator</span><br><span class="line">&#125;)</span><br><span class="line">outRdd.take(<span class="number">10</span>).foreach(println)</span><br><span class="line"><span class="comment">// println(outRdd.count())</span></span><br></pre></td></tr></table></figure>
<ul>
<li>优点： 代码简洁，容易理解，性能高; </li>
<li>缺点： 需要学习RDD相关知识。</li>
</ul>
<h1 id="姿势3-使用Dataframe-继承UserDefinedAggregateFunction"><a href="#姿势3-使用Dataframe-继承UserDefinedAggregateFunction" class="headerlink" title="姿势3: 使用Dataframe(继承UserDefinedAggregateFunction)"></a>姿势3: 使用Dataframe(继承UserDefinedAggregateFunction)</h1><p>假设用户比较熟悉Dataframe操作，还可以通过继承<code>UserDefinedAggregateFunction</code>类编写一个完整的UDAF：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// part1: UDAF:</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DfUdaf</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">StringType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Map[String,Null] 当Set用了</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">"idfa_idfv"</span>, <span class="type">MapType</span>(<span class="type">StringType</span>, <span class="type">NullType</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">MapType</span>(<span class="type">StringType</span>, <span class="type">NullType</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer.update(<span class="number">0</span>, <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Null</span>]())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> map = buffer.getAs[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Null</span>]](<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> value = input.getAs[<span class="type">String</span>](<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> idfa_idfv = <span class="type">UdfUtils</span>.str2idfa(value)</span><br><span class="line">    <span class="keyword">if</span> (idfa_idfv.isDefined) &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>, map ++ <span class="type">Map</span>(idfa_idfv.get -&gt; <span class="literal">null</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> map1 = buffer1.getAs[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Null</span>]](<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> map2 = buffer2.getAs[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Null</span>]](<span class="number">0</span>)</span><br><span class="line">    buffer1.update(<span class="number">0</span>, map1 ++ map2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = buffer.getAs[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Null</span>]](<span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// part2: main函数:</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.appName(<span class="string">"UdafDemo"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sqlProvider = <span class="keyword">new</span> <span class="type">TDWSQLProvider</span>(spark, user, pass, db)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// val rddProvider = TDWProvider(spark.sparkContext, user, pass, db) // 这个返回rdd</span></span><br><span class="line">    <span class="keyword">val</span> inDf = sqlProvider.table(<span class="string">"t_dw_dc0xxxx"</span>, <span class="type">Array</span>(<span class="string">"p_2019042100"</span>))</span><br><span class="line">    println(<span class="string">"getNumPartitions:"</span>)</span><br><span class="line">    println(inDf.rdd.getNumPartitions)</span><br><span class="line"></span><br><span class="line">    spark.udf.register(<span class="string">"collect_idfa"</span>, <span class="type">DfUdaf</span>)</span><br><span class="line">    inDf.createOrReplaceTempView(<span class="string">"t1"</span>)</span><br><span class="line">    <span class="keyword">val</span> outDf = spark.sql(<span class="string">""</span> +</span><br><span class="line">      <span class="string">"select uin,idfa_idfv "</span> +</span><br><span class="line">      <span class="string">"from "</span> +</span><br><span class="line">      <span class="string">"(select uin,collect_idfa(value) as vmap from t1 group by uin) a "</span> +</span><br><span class="line">      <span class="string">"lateral view explode(vmap) tt as idfa_idfv,n"</span> +</span><br><span class="line">      <span class="string">""</span>)</span><br><span class="line">    outDf.take(<span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure>
<p>优点: 可以直接在sql中引用，重用性高，性能高;<br>缺点: 开发成本高，只支持scala，需要编译。</p>
<h1 id="姿势4：-使用Dataset（继承Aggregator）"><a href="#姿势4：-使用Dataset（继承Aggregator）" class="headerlink" title="姿势4： 使用Dataset（继承Aggregator）"></a>姿势4： 使用Dataset（继承Aggregator）</h1><p>如果用户对于Dataset的api比较熟悉，可以继承Aggregator开发UDAF:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// part1: UDAF:</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DsUdaf</span>[<span class="type">IN</span>](<span class="params">val f: <span class="type">IN</span> =&gt; <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">IN</span>, <span class="type">Set</span>[<span class="type">String</span>], <span class="title">Set</span>[<span class="type">String</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Set</span>[<span class="type">String</span>] = <span class="type">Set</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buf: <span class="type">Set</span>[<span class="type">String</span>], a: <span class="type">IN</span>): <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> idfa_idfv = <span class="type">UdfUtils</span>.str2idfa(f(a))</span><br><span class="line">    buf ++ idfa_idfv</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Set</span>[<span class="type">String</span>], b2: <span class="type">Set</span>[<span class="type">String</span>]): <span class="type">Set</span>[<span class="type">String</span>] = b1 ++ b2</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Set</span>[<span class="type">String</span>]): <span class="type">Set</span>[<span class="type">String</span>] = reduction</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Set</span>[<span class="type">String</span>]] = <span class="type">Encoders</span>.kryo[<span class="type">Set</span>[<span class="type">String</span>]]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Set</span>[<span class="type">String</span>]] = <span class="type">Encoders</span>.kryo[<span class="type">Set</span>[<span class="type">String</span>]]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// part2: main函数:</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.appName(<span class="string">"UdfDemo"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sqlProvider = <span class="keyword">new</span> <span class="type">TDWSQLProvider</span>(spark, user, pass, db)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// val rddProvider = TDWProvider(spark.sparkContext, user, pass, db) // 这个返回rdd</span></span><br><span class="line">    <span class="keyword">val</span> inDf = sqlProvider.table(<span class="string">"t_dw_dc0xxxx"</span>, <span class="type">Array</span>(<span class="string">"p_2019042100"</span>))</span><br><span class="line">    println(<span class="string">"getNumPartitions:"</span>)</span><br><span class="line">    println(inDf.rdd.getNumPartitions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    inDf.createOrReplaceTempView(<span class="string">"t1"</span>)</span><br><span class="line">    <span class="keyword">val</span> df2 = spark.sql(<span class="string">"select uin,value from t1"</span>)</span><br><span class="line">    df2.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> inDS = df2.as[<span class="type">UinValue</span>]</span><br><span class="line">    <span class="comment">// inDS.take(10).foreach(println)</span></span><br><span class="line">    <span class="keyword">val</span> outDs: <span class="type">Dataset</span>[(<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>])] = inDS.groupByKey(_.uin).agg(<span class="keyword">new</span> <span class="type">DsUdaf</span>[<span class="type">UinValue</span>](_.value).toColumn)</span><br><span class="line">    <span class="comment">// outDs.take(10).foreach(println)</span></span><br><span class="line">    <span class="keyword">val</span> ds2 = outDs.flatMap(pair =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> uin = pair._1</span><br><span class="line">      <span class="keyword">val</span> idfa_set = pair._2</span><br><span class="line">      idfa_set.map(idfa =&gt; (uin, idfa))</span><br><span class="line">    &#125;)</span><br><span class="line">    ds2.printSchema()</span><br><span class="line">    ds2.take(<span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure>
<p>其中<code>Encoder</code>部分由于还不支持Set集合类型，可以使用kryo序列化成二进制。（更多Encoder相关参见:<a href="http://xiaoyue26.github.io/2019/04/27/2019-04/spark%E4%B8%AD%E7%9A%84encoder/">http://xiaoyue26.github.io/2019/04/27/2019-04/spark%E4%B8%AD%E7%9A%84encoder/</a> ）</p>
<p>优点: 类型安全，继承Aggregator开发的成本略小于继承UserDefinedAggregateFunction;<br>缺点: 只支持scala，需要编译。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文总结了在Rdd,Dataframe,Dataset三种api下编写UDAF的方法（三种api的对比参见<a href="http://xiaoyue26.github.io/2019/04/29/2019-04/spark%E4%B8%ADRDD%EF%BC%8CDataframe%EF%BC%8CDataSet%E5%8C%BA%E5%88%AB%E5%AF%B9%E6%AF%94/">http://xiaoyue26.github.io/2019/04/29/2019-04/spark%E4%B8%ADRDD%EF%BC%8CDataframe%EF%BC%8CDataSet%E5%8C%BA%E5%88%AB%E5%AF%B9%E6%AF%94/</a> ），以及使用UDF模拟UDAF功能的方法。大家可以根据自己熟悉的api和需求选择。</p>
<ul>
<li>如果不在意性能：用<code>collect_set</code>+<code>UDF</code>模拟一个；(姿势1)</li>
<li>如果在意性能，但是只用一次: 可以直接用RDD的<code>combineByKey</code>，代码较短；（姿势2） </li>
<li>如果在意性能，而且会反复复用: 建议使用Dataframe，继承<code>UserDefinedAggregateFunction</code>编写一个UDAF。（姿势3）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/05/11/2019-05/spark%E4%B8%AD%E7%BC%96%E5%86%99UDAF%E7%9A%844%E7%A7%8D%E5%A7%BF%E5%8A%BF/" data-id="ck96cxpps00igmaamhxv76yy4" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-05/将pyspark中的UDF加速50" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/08/2019-05/%E5%B0%86pyspark%E4%B8%AD%E7%9A%84UDF%E5%8A%A0%E9%80%9F50/" class="article-date">
  <time datetime="2019-05-08T14:39:03.000Z" itemprop="datePublished">2019-05-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/08/2019-05/%E5%B0%86pyspark%E4%B8%AD%E7%9A%84UDF%E5%8A%A0%E9%80%9F50/">将pyspark中的UDF加速50%</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><blockquote>
<p>调用jar中的UDF，减少python与JVM的交互，简单banchmark下对于50G数据集纯map处理可以减少一半处理时间。<br>牺牲UDF部分的开发时间，尽量提高性能。<br>以接近纯python的开发成本，获得逼近纯scala的性能。兼顾性能和开发效率。</p>
</blockquote>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>  当遇到sql无法直接处理的数据时(比如加密解密、thrift解析操作二进制)，我们需要自定义函数(UDF)来进行处理。出于开发效率的考虑，我们一般会选择tesla平台，使用pyspark脚本。</p>
<h1 id="Before-最简单的UDF"><a href="#Before-最简单的UDF" class="headerlink" title="Before: 最简单的UDF"></a>Before: 最简单的UDF</h1><p>一个最简单的UDF处理大致如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2idfa</span><span class="params">(txt)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        txtDecoded = base64.b64decode(txt)</span><br><span class="line">        bytesWithSalt = bytes(txtDecoded)</span><br><span class="line">        <span class="comment"># 省略实际处理代码</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'dump_data'</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'error here'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'-1#-1'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spark = SparkSession.builder.appName(app_name).getOrCreate()</span><br><span class="line">    in_provider = TDWSQLProvider(spark, user=user, passwd=passwd, db=db_name)</span><br><span class="line"></span><br><span class="line">    in_df = in_provider.table(<span class="string">'t_dw_dc0xxxx'</span>, [<span class="string">'p_2019042100'</span>])  <span class="comment"># 分区数组</span></span><br><span class="line">    print(in_df.columns)</span><br><span class="line">    in_df.createOrReplaceTempView(<span class="string">"t1"</span>)</span><br><span class="line">    <span class="comment"># 1. 注册udf:</span></span><br><span class="line">    spark.udf.register(<span class="string">"str2idfa"</span>, str2idfa, StringType())</span><br><span class="line">    <span class="comment"># 2. 在sql中使用:</span></span><br><span class="line">    out_t1 = spark.sql(<span class="string">'''select uin</span></span><br><span class="line"><span class="string">        ,str2idfa(value) as idfa_idfv</span></span><br><span class="line"><span class="string">        from t1</span></span><br><span class="line"><span class="string">        '''</span>)</span><br><span class="line">    print(out_t1.columns)</span><br><span class="line">    print(out_t1.take(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h2 id="底层实现原理"><a href="#底层实现原理" class="headerlink" title="底层实现原理"></a>底层实现原理</h2><img src="/images/2019-05/pyspark_call.png" class="" width="800" height="1200" title="pyspark_call">
<p>  如上图所示，pyspark并没有像dpark一样用python重新实现一个计算引擎，依旧是复用了scala的jvm计算底层，只是用py4j架设了一条python进程和jvm互相调用的桥梁。<br>  <code>driver</code>:  pyspark脚本和sparkContext的jvm使用py4j相互调用;<br>  <code>executor</code>: 由于driver帮忙把spark算子封装好了，执行计划也生成了字节码，一般情况下不需要python进程参与，仅当需要运行UDF(含lambda表达式形式)时，将它委托给python进程处理(DAG图中的<code>BatchEvalPython</code>步骤)，此时JVM和python进程使用socket通信。</p>
<p>   上述使用简单UDF时的pyspark由于需要使用UDF，因此DAG图中有<code>BatchEvalPython</code>步骤:<br>   <img src="/images/2019-05/py_udf.png" class="" width="400" height="400" title="py_udf"></p>
<h2 id="BatchEvalPython过程"><a href="#BatchEvalPython过程" class="headerlink" title="BatchEvalPython过程"></a>BatchEvalPython过程</h2><p>参考源码：<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala</a><br>可以看到和这个名字一样直白，它就是每次取100条数据让python进程帮忙处理一下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第58行:</span></span><br><span class="line"><span class="comment">// Input iterator to Python: input rows are grouped so we send them in batches to Python.</span></span><br><span class="line">    <span class="comment">// For each row, add it to the queue.</span></span><br><span class="line">    <span class="keyword">val</span> inputIterator = iter.map &#123; row =&gt;</span><br><span class="line">      <span class="keyword">if</span> (needConversion) &#123;</span><br><span class="line">        <span class="type">EvaluatePython</span>.toJava(row, schema)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// fast path for these types that does not need conversion in Python</span></span><br><span class="line">        <span class="keyword">val</span> fields = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Any</span>](row.numFields)</span><br><span class="line">        <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt; row.numFields) &#123;</span><br><span class="line">          <span class="keyword">val</span> dt = dataTypes(i)</span><br><span class="line">          fields(i) = <span class="type">EvaluatePython</span>.toJava(row.get(i, dt), dt)</span><br><span class="line">          i += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">        fields</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.grouped(<span class="number">100</span>).map(x =&gt; pickle.dumps(x.toArray))</span><br></pre></td></tr></table></figure>


<p>   由于我们的计算任务一般耗时瓶颈在于executor端的计算而不是driver，因此应该考虑尽量减少executor端调用python代码的次数从而优化性能。</p>
<p>参考源码：<a href="https://github.com/apache/spark/blob/master/python/pyspark/java_gateway.py" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/python/pyspark/java_gateway.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 大概<span class="number">135</span>行的地方:</span><br><span class="line"><span class="comment"># Import the classes used by PySpark</span></span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.SparkConf"</span>)</span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.api.java.*"</span>)</span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.api.python.*"</span>)</span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.ml.python.*"</span>)</span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.mllib.api.python.*"</span>)</span><br><span class="line"><span class="comment"># TODO(davies): move into sql</span></span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.sql.*"</span>)</span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.sql.api.python.*"</span>)</span><br><span class="line">java_import(gateway.jvm, <span class="string">"org.apache.spark.sql.hive.*"</span>)</span><br><span class="line">java_import(gateway.jvm, <span class="string">"scala.Tuple2"</span>)</span><br></pre></td></tr></table></figure>
<p>pyspark可以把很多常见的运算封装到JVM中,但是显然不包括我们的UDF。<br>所以一个很自然的思路就是把我们的UDF也封到JVM中。</p>
<h1 id="After-调用JAR包中UDF"><a href="#After-调用JAR包中UDF" class="headerlink" title="After: 调用JAR包中UDF"></a>After: 调用JAR包中UDF</h1><p>首先我们需要用scala重写一下UDF:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UdfUtils</span> <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Idfa</span>(<span class="params">idfa: <span class="type">String</span>, idfv: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(<span class="type">V</span>: <span class="type">String</span>, defV: <span class="type">String</span>) =</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">V</span> == <span class="literal">null</span>) defV <span class="keyword">else</span> <span class="type">V</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = coalesce(idfa, <span class="string">"-1"</span>) + <span class="string">"#"</span> + coalesce(idfv, <span class="string">"-1"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">str2idfa</span></span>(txt: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> decodeTxt: <span class="type">Array</span>[<span class="type">Byte</span>] = <span class="type">Base64</span>.getDecoder.decode(txt)</span><br><span class="line">      <span class="comment">// TODO 省略一些处理逻辑</span></span><br><span class="line">      <span class="keyword">val</span> str = <span class="string">"after_some_time"</span></span><br><span class="line">      <span class="keyword">val</span> gson = <span class="keyword">new</span> <span class="type">Gson</span>()</span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">JsonReader</span>(<span class="keyword">new</span> <span class="type">StringReader</span>(str))</span><br><span class="line">      reader.setLenient(<span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">val</span> idfaType: <span class="type">Type</span> = <span class="keyword">new</span> <span class="type">TypeToken</span>[<span class="type">Idfa</span>]() &#123;&#125;.getType</span><br><span class="line">      <span class="type">Some</span>(gson.fromJson(reader, idfaType).toString)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        println(txt)</span><br><span class="line">        e.printStackTrace()</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 关键是这里把普通函数转成UDF:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">str2idfaUDF</span></span>: <span class="type">UserDefinedFunction</span> = udf(str2idfa _)</span><br></pre></td></tr></table></figure>

<p>然后在pyspark脚本里调用jar包中的UDF:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> pytoolkit <span class="keyword">import</span> TDWSQLProvider, TDWUtil, TDWProvider</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SQLContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession, Row</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, LongType, StringType, StructField, IntegerType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf, struct, array</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> Column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_java_column</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.column <span class="keyword">import</span> _to_seq</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2idfa</span><span class="params">(col)</span>:</span></span><br><span class="line">    _str2idfa = sc._jvm.com.tencent.kandian.utils.UdfUtils.str2idfaUDF()</span><br><span class="line">    <span class="keyword">return</span> Column(_str2idfa.apply(_to_seq(sc, [col], _to_java_column)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(app_name).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    in_provider = TDWSQLProvider(spark, user=user, passwd=passwd, db=db_name)</span><br><span class="line">    in_df = in_provider.table(<span class="string">'t_dw_dcxxxx'</span>, [<span class="string">'p_2019042100'</span>])  <span class="comment"># 分区数组</span></span><br><span class="line">    print(in_df.columns)</span><br><span class="line">    in_df.createOrReplaceTempView(<span class="string">"t1"</span>)</span><br><span class="line">    out_t1 = in_df.select(col(<span class="string">'uin'</span>)</span><br><span class="line">                          , str2idfa(col(<span class="string">"value"</span>))) <span class="comment"># 直接使用scala的udf,节省43%时间,减少两个transform</span></span><br><span class="line">    print(out_t1.columns)</span><br><span class="line">    print(out_t1.take(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>其中<code>_jvm</code>变量是<code>sparkContext</code>中<code>JVMView</code>对象的名字,此外sc中还有<code>_gateway</code>变量以连接JVM中的<code>GatawayServer</code>。<br>提交时，在tesla上的配置<code>spark-conf</code>jar包路径:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.extraClassPath&#x3D;pipe-udf-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">spark.executor.extraClassPath&#x3D;pipe-udf-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>
<p>同时在依赖包文件中上传jar包。</p>
<p>这样一通操作之后，DAG图变成了这样:</p>
<img src="/images/2019-05/py_jar.png" class="" width="400" height="400" title="py_jar">

<p>可以看到比之前少了两个transform,没有了<code>BatchEvalPython</code>，也少了一个<code>WholeStageCodeGen</code>。<br>经过简单banchmark，对于50G数据集纯map处理。<br>第一种方案：大约13分钟；<br>第二种方案：大约7分钟。<br>第二种方案大约能节省一半的时间，并且进一步测试使用scala完全重写整个计算，运行时间和第二种方案接近，也大约需要7分钟。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>   在pyspark中尽量使用spark算子和spark-sql，同时尽量将UDF(含lambda表达式形式)封装到一个地方减少JVM和python脚本的交互。<br>   由于<code>BatchEvalPython</code>过程每次处理100行，也可以把多行聚合成一行减少交互次数。<br>   最后还可以把UDF部分用scala重写打包成jar包，其他部分则保持python脚本以获得不用编译随时修改的灵活性，以兼顾性能和开发效率。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/05/08/2019-05/%E5%B0%86pyspark%E4%B8%AD%E7%9A%84UDF%E5%8A%A0%E9%80%9F50/" data-id="ck96cxpps00ijmaam3j3lf6mt" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-04/spark中RDD，Dataframe，DataSet区别对比" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/29/2019-04/spark%E4%B8%ADRDD%EF%BC%8CDataframe%EF%BC%8CDataSet%E5%8C%BA%E5%88%AB%E5%AF%B9%E6%AF%94/" class="article-date">
  <time datetime="2019-04-29T02:16:54.000Z" itemprop="datePublished">2019-04-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/29/2019-04/spark%E4%B8%ADRDD%EF%BC%8CDataframe%EF%BC%8CDataSet%E5%8C%BA%E5%88%AB%E5%AF%B9%E6%AF%94/">spark中RDD，Dataframe，DataSet区别对比</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="RDD，Dataframe，DataSet的定义"><a href="#RDD，Dataframe，DataSet的定义" class="headerlink" title="RDD，Dataframe，DataSet的定义"></a>RDD，Dataframe，DataSet的定义</h1><p><code>RDD</code>: immutable、spark的基础数据集。底层api； 1.0+, 存放java/scala对象;<br><code>Dataframe</code>: immutable、多了列名、Catalyst优化。高级api;  1.3+, 存放row对象;(有列名)<br><code>Dataset</code>: 比Dataframe多类型安全。高级api。 1.6+, 存放java/scala对象(暴露更多信息给编译期)<br>可以理解成DataSet每行存一个大对象。（比如样例中的<code>DataSet[Person]</code>）</p>
<p>DataSet一般比DataFrame慢一点点，多了类型安全的开销（<code>Row</code>=&gt;<code>Java类型</code>）</p>
<h2 id="这里说的类型安全是什么？"><a href="#这里说的类型安全是什么？" class="headerlink" title="这里说的类型安全是什么？"></a>这里说的类型安全是什么？</h2><blockquote>
<p>是编译期类型安全</p>
</blockquote>
<p>Dataframe: 访问不存在的列名=&gt;运行时报错; （非类型安全）<br>DataSet:   访问不存在的列名=&gt;编译时报错。 （类型安全）</p>
<p>DataSet的中每个元素都是case class之类的,完全定义类型的,因此编译时就能确定schema合法性。</p>
<p><code>Dataset[Row]</code> = <code>DataFrame</code><br>因此Row可以看做非编译期类型安全的对象。</p>
<h2 id="java-scala对象-lt-gt-DataFrame的row对象-DataSet的元素"><a href="#java-scala对象-lt-gt-DataFrame的row对象-DataSet的元素" class="headerlink" title="java/scala对象 &lt;=&gt; DataFrame的row对象/DataSet的元素:"></a>java/scala对象 &lt;=&gt; DataFrame的row对象/DataSet的元素:</h2><ol>
<li>对于dataframe: 传入StructType;</li>
<li>对于dataSet:  使用Encoder。 </li>
</ol>
<p>类似于hive的反序列化:</p>
<h3 id="hive反序列化原理"><a href="#hive反序列化原理" class="headerlink" title="hive反序列化原理:"></a>hive反序列化原理:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HDFS files –&gt; InputFileFormat –&gt; &lt;key, value&gt; –&gt; Deserializer –&gt; Row object</span><br></pre></td></tr></table></figure>
<h3 id="hive序列化原理-Serialize"><a href="#hive序列化原理-Serialize" class="headerlink" title="hive序列化原理(Serialize):"></a>hive序列化原理(<code>Serialize</code>):</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Row object –&gt; Serializer –&gt; &lt;key, value&gt; –&gt; OutputFileFormat –&gt; HDFS files</span><br></pre></td></tr></table></figure>

<p><code>Encoder</code>: <code>Dataset[Row]</code> -&gt; <code>Dataset[T]</code><br>JVM对象和非堆自定义内存二进制数据<br>Encoders生成二进制代码来跟非堆数据交互，并且提供有需访问给独立的参数而不是对整个对象反序列化。</p>
<h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>RDD: 没有优化, 程序员自己保证RDD运算是最优的;<br>DataFrame: 走catalyst编译优化,类似于Sql的优化。根据成本模型，逻辑执行计划优化成物理执行计划。<br>DataSet: 同DataFrame. </p>
<p>强调一点,DataFrame底层也是用的RDD实现，因此如果程序员足够牛逼，理论上执行计划能写得比DataFrame的计划好。</p>
<h1 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h1><p>shuffle的时候、或者cache写内存、磁盘的时候，需要序列化。</p>
<p><code>RDD</code>: 使用java序列化(或者kryo)成二进制; (成本高)<br><code>DataFrame</code>: Tungsten计划优化。序列化到堆外内存，然后无须反序列化，直接根据schema操作二进制运算。(因为DataFrame比RDD多一个schema信息)<br><a href="https://github.com/hustnn/TungstenSecret" target="_blank" rel="noopener">https://github.com/hustnn/TungstenSecret</a><br><code>DataSet</code>: 基本同DataFrame，多了Encoder的概念。访问某列的时候，可以只反序列化那个局部的二进制。</p>
<h2 id="Tungsten-binary-format"><a href="#Tungsten-binary-format" class="headerlink" title="Tungsten binary format"></a>Tungsten binary format</h2><p>钨丝计划使用的二进制格式</p>
<h1 id="垃圾收集"><a href="#垃圾收集" class="headerlink" title="垃圾收集"></a>垃圾收集</h1><p>RDD: 由于上一节中的序列化，gc压力较大；<br>DataFrame: 放堆外内存，无需jvm对象头开销，无gc；<br>DataSet: 同df.</p>
<h1 id="钨丝计划的秘密："><a href="#钨丝计划的秘密：" class="headerlink" title="钨丝计划的秘密："></a>钨丝计划的秘密：</h1><p><a href="https://github.com/hustnn/TungstenSecret" target="_blank" rel="noopener">https://github.com/hustnn/TungstenSecret</a></p>
<p>总结一下钨丝计划的3大优化：</p>
<ol>
<li>内存管理、直接操作二进制数据：放堆外内存(避免gc开销)，不用jvm对象(减少对象头开销)。</li>
<li>缓存友好的计算: 考虑存储体系;(flink也有) 对指针排序，根据schema访问key的二进制。(估计key只能是原生类型，其他类就得反序列化了)</li>
<li>code generation：充分利用最新的编译期和cpu性能：把jvm对象转到堆外unsafeRow，以便利用第一点。</li>
</ol>
<p>比如8B的String的对象头有40B开销。<br>UnsafeShuffleManager： 直接在serialized binary data上sort而不是java objects</p>
<h2 id="wholeStageCodeGen"><a href="#wholeStageCodeGen" class="headerlink" title="wholeStageCodeGen"></a>wholeStageCodeGen</h2><p>把transform的很多步骤，合并成一个步骤，使用字符串插值生成一份重写后的代码(逼近于手写最优解)，然后用Janino(微型运行时嵌入式java编译器)编译成字节码。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/04/29/2019-04/spark%E4%B8%ADRDD%EF%BC%8CDataframe%EF%BC%8CDataSet%E5%8C%BA%E5%88%AB%E5%AF%B9%E6%AF%94/" data-id="ck96cxppo00i6maam8bk2eis5" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-04/spark中的encoder" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/27/2019-04/spark%E4%B8%AD%E7%9A%84encoder/" class="article-date">
  <time datetime="2019-04-27T03:23:26.000Z" itemprop="datePublished">2019-04-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/27/2019-04/spark%E4%B8%AD%E7%9A%84encoder/">spark中的encoder</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>参考资料:<br><a href="https://stackoverflow.com/questions/53949497/why-a-encoder-is-needed-for-creating-dataset-in-spark" target="_blank" rel="noopener">https://stackoverflow.com/questions/53949497/why-a-encoder-is-needed-for-creating-dataset-in-spark</a><br><code>It also uses less memory than Kryo/Java serialization.</code></p>
<h1 id="What-Encoder是啥"><a href="#What-Encoder是啥" class="headerlink" title="What: Encoder是啥?"></a>What: Encoder是啥?</h1><p>所有<code>DataSet</code>都需要<code>Encoder</code>。</p>
<p><code>Encoder</code>是spark-sql用来序列化/反序列化的一个类。主要用于<code>DataSet</code>。<br>本质上每次调用<code>toDS()</code>函数的时候都调用了<code>Encoder</code>，不过有时候我们察觉不到，因为用了隐式调用(<code>import spark.implicits._</code>)。<br>可以直接看<code>Encoder</code>源码注释中的样例:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val ds &#x3D; Seq(1, 2, 3).toDS() &#x2F;&#x2F; implicitly provided (spark.implicits.newIntEncoder)</span><br><span class="line">&#x2F;&#x2F; 将数字(JVM对象)转换为DataSet中的元素</span><br><span class="line">&#x2F;&#x2F; 这里由于是常见的原始类型，所以spark提供了隐式encoder的调用，隐藏了这些细节。</span><br></pre></td></tr></table></figure>
<p>Encoder将jvm转换为堆外内存二进制，使用成员位置信息，降低反序列化的范围（反序列化需要的列即可）。<br>// (类似于Hive中的反序列化,把kv转换为row)</p>
<p>Encoder不要求线程安全。</p>
<h1 id="Why-为啥用Encoder"><a href="#Why-为啥用Encoder" class="headerlink" title="Why: 为啥用Encoder?"></a>Why: 为啥用Encoder?</h1><p>stackoverflow上说encoder消耗更少的内存。因为kryo把<code>dataSet</code>中的所有行都变成了一个打平的二进制对象。<br><code>10x faster than Kryo serialization (Java serialization orders of magnitude slower)</code><br>DataFrame本质上是DataSet[Row]，用的固定是RowEncoder，所以不需要传Encoder。<br>Encoder底层是钨丝计划的堆外内存优化，节省了jvm对象头、反序列化、gc的开销。</p>
<h1 id="When-啥时候使用Encoder"><a href="#When-啥时候使用Encoder" class="headerlink" title="When: 啥时候使用Encoder"></a>When: 啥时候使用Encoder</h1><p><code>Encoder</code>适用于原始类型、case class对象(因为有默认的apply/unapply方法)、spark-sql类型。</p>
<p>Encoder支持的类型非常多，不支持的情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 如果类型是javabean，类的成员如果是容器，只能是List，不能是其他容器（还没有实现）;</span><br><span class="line">2. 不支持大于5的Tuple；</span><br><span class="line">3. 不支持&#96;Option&#96;；</span><br><span class="line">4. 不支持&#96;null&#96;值的&#96;case class&#96;。</span><br></pre></td></tr></table></figure>
<p>不支持的时候，可以把不支持的部分用kyro-Encoder，相当于不支持的部分直接当做一个二进制，不享受优化，但其他不支持部分可以享受优化。</p>
<h1 id="How-怎么使用Encoder"><a href="#How-怎么使用Encoder" class="headerlink" title="How: 怎么使用Encoder:"></a>How: 怎么使用Encoder:</h1><p>显式： 使用<code>Encoders</code>类(类似于utils)的静态工厂方法;<br>隐式：<code>import spark.implicits._</code>: 原始类型和<code>Product</code>类型(也就是<code>case class</code>)可以直接隐式支持。</p>
<h2 id="1-创建DataSet时显式使用"><a href="#1-创建DataSet时显式使用" class="headerlink" title="1. 创建DataSet时显式使用:"></a>1. 创建DataSet时显式使用:</h2><p>源码注释中的样例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// eg1: String:</span></span><br><span class="line">List&lt;String&gt; data = Arrays.asList(<span class="string">"abc"</span>, <span class="string">"abc"</span>, <span class="string">"xyz"</span>);</span><br><span class="line">Dataset&lt;String&gt; ds = context.createDataset(data, Encoders.STRING());</span><br><span class="line"><span class="comment">// eg2: 复合Tuple:</span></span><br><span class="line">Encoder&lt;Tuple2&lt;Integer, String&gt;&gt; encoder2 = Encoders.tuple(Encoders.INT(), Encoders.STRING());</span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; data2 = Arrays.asList(<span class="keyword">new</span> scala.Tuple2(<span class="number">1</span>, <span class="string">"a"</span>);</span><br><span class="line">Dataset&lt;Tuple2&lt;Integer, String&gt;&gt; ds2 = context.createDataset(data2, encoder2);</span><br></pre></td></tr></table></figure>
<h2 id="2-创建DataSet时隐式使用"><a href="#2-创建DataSet时隐式使用" class="headerlink" title="2. 创建DataSet时隐式使用:"></a>2. 创建DataSet时隐式使用:</h2><p>看createDataset的签名:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def createDataset[T](data: RDD[T])(implicit arg0: Encoder[T]): Dataset[T]</span><br><span class="line">&#x2F;&#x2F; Creates a Dataset from an RDD of a given type.</span><br></pre></td></tr></table></figure>
<p>所以Encoder其实可以隐式传:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val ds &#x3D; Seq(1, 2, 3).toDS() &#x2F;&#x2F; implicitly provided</span><br></pre></td></tr></table></figure>
<h2 id="3-UDAF中使用"><a href="#3-UDAF中使用" class="headerlink" title="3. UDAF中使用:"></a>3. UDAF中使用:</h2><p><a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala</a></p>
<p>当我们为<code>DataSet</code>定义<code>UDAF</code>的使用。<br>语义上: 因为涉及到数据转换，不可避免地会需要使用<code>Encoder</code>，这个时候是显式使用。<br>语法上: 由于继承了<code>Aggregator</code>也必须使用<code>Encoder</code>。</p>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">trait Encoder[T] extends Serializable &#123;</span><br><span class="line">  &#x2F;** Returns the schema of encoding this type of object as a Row. *&#x2F;</span><br><span class="line">  def schema: StructType</span><br><span class="line">  &#x2F;**</span><br><span class="line">   * A ClassTag that can be used to construct and Array to contain a collection of &#96;T&#96;.*&#x2F;</span><br><span class="line">  def clsTag: ClassTag[T]</span><br><span class="line">  &#x2F;&#x2F; 存了ClassTag的话，就能在运行时构建泛型的数组了。</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>TypeTag</code>: 相当于scala以前的<code>Manifest</code>,用于存储泛型参数的实际类型。(泛型参数的实际类型运行时会被JVM擦除，有了<code>TypeTag</code>就能在运行时获得实际类型了)<br><code>ClassTag</code>: 相当于scala以前的<code>ClassManifest</code>,功能大致同上，但存得少些，比如如果是泛型的泛型，参数是泛型数组<code>List[T]</code>,<code>TypeTag</code>能全部存下，<code>ClassTag</code>就存一个<code>List</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">classTag[List[Int]]</span><br><span class="line">&#x2F;&#x2F;scala.reflect.ClassTag[List[Int]] &#x3D;↩</span><br><span class="line">&#x2F;&#x2F;        ClassTag[class scala.collection.immutable.List]</span><br><span class="line">typeTag[List[Int]]</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F; reflect.runtime.universe.TypeTag[List[Int]] &#x3D; TypeTag[scala.List[Int]]</span><br></pre></td></tr></table></figure>


<h2 id="ExpressionEncoder"><a href="#ExpressionEncoder" class="headerlink" title="ExpressionEncoder"></a>ExpressionEncoder</h2><p>Encoder的内置唯一实现类。<br><code>jvm对象&lt;=&gt;内部行格式</code>: 钨丝计划unsafeRow、expressions提取case class的变量名。<br>可以支持Tupple但不支持<code>Option</code>和<code>和null</code>值的<code>case class</code>。</p>
<p>它会生成变量名name和位置的绑定，以便钨丝计划的<code>code gen</code>使用<code>unsafe row</code>.<br>Tupple最多到5.</p>
<p><code>Serializer</code>: raw object=&gt;InternalRow, 用expression解析提取对象值；<br><code>Deserializer</code>: InternalRow=&gt;raw object,用expression构造对象。<br>因为unsafeRow是二进制存放在堆外，所以转换成row看做序列化。</p>
<h3 id="Encoders"><a href="#Encoders" class="headerlink" title="Encoders"></a>Encoders</h3><p>(注意比Encoder多一个s)<br>提供了很多静态工厂方法获得Encoder(实际上目前获得的都是<code>ExpressionEncoder</code>)<br>大致可以分为几类:</p>
<ol>
<li>java原始类型:  <code>Encoders.BOOLEAN</code>等</li>
<li>scala原始类型: <code>Encoders.scalaBoolean</code>等.(多一个scala前缀)</li>
<li>javaBean类型: <code>bean[T](beanClass: Class[T])</code>。但目前成员只支持List容器，不支持其他的容器。支持原始类型或嵌套javaBean。</li>
<li>kryo序列化类型: <code>kryo[T: ClassTag]</code>；</li>
<li>java序列化类型: <code>javaSerialization[T: ClassTag]</code>；</li>
<li>Tuple类型: 从Tuple2到Tuple5.</li>
<li>Product类型: 也就是<code>case class</code>.</li>
</ol>
<p>其中前三种是直接调用<code>ExpressionEncoder</code>，第四第五种本质上是间接调用了<code>ExpressionEncoder</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ExpressionEncoder[T](</span><br><span class="line">      schema &#x3D; new StructType().add(&quot;value&quot;, BinaryType),</span><br><span class="line">      flat &#x3D; true,</span><br><span class="line">      serializer &#x3D; Seq(</span><br><span class="line">        EncodeUsingSerializer(</span><br><span class="line">          BoundReference(0, ObjectType(classOf[AnyRef]), nullable &#x3D; true), kryo &#x3D; useKryo)),</span><br><span class="line">      deserializer &#x3D;</span><br><span class="line">        DecodeUsingSerializer[T](</span><br><span class="line">          Cast(GetColumnByOrdinal(0, BinaryType), BinaryType),</span><br><span class="line">          classTag[T],</span><br><span class="line">          kryo &#x3D; useKryo),</span><br><span class="line">      clsTag &#x3D; classTag[T]</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>因此第四第五后两种序列化本质上是把整个对象看做一个二进制类型，不利于后续优化和减少反序列化。</p>
<p>原始类型还包括:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java的:</span><br><span class="line">byte,short,int,long,float,double,java.math.BigDecimal,java.sql.Date,java.sql.Timestamp</span><br><span class="line">scala的:</span><br><span class="line">Array[Byte],byte,short,int,long,float,double</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/04/27/2019-04/spark%E4%B8%AD%E7%9A%84encoder/" data-id="ck96cxppp00i9maam5qixbnbe" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-04/2019年的人们如何生成HTTPS证书" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/14/2019-04/2019%E5%B9%B4%E7%9A%84%E4%BA%BA%E4%BB%AC%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90HTTPS%E8%AF%81%E4%B9%A6/" class="article-date">
  <time datetime="2019-04-14T11:59:09.000Z" itemprop="datePublished">2019-04-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/http/">http</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/14/2019-04/2019%E5%B9%B4%E7%9A%84%E4%BA%BA%E4%BB%AC%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90HTTPS%E8%AF%81%E4%B9%A6/">2019年的人们如何生成HTTPS证书</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>由于配置相关的教程总是有年限限制，过期就不能用了，本教程至少保证2019.4.14还可用。<br>环境: centos,nginx,chrome<br>备注: 可以避免chrome的<code>NET::ERR_CERT_COMMON_NAME_INVALID</code>错误。</p>
<p><strong>摘要</strong></p>
<blockquote>
<p>从HTTP升级到HTTPS: 用openssl命令创建本地的CA,然后自签证书，然后配置到nginx中，最后信任一下本地CA即可。</p>
</blockquote>
<h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><p>从HTTP协议升级到HTTPS。<br>用HTTPS可以防止会话内容被拦截解析,同时防止中间人攻击，防止他人伪装称你的网站，欺骗你的客户。<br>SSL协议的详细含义可以参见:<br><a href="https://xiaoyue26.github.io/2018/09/26/2018-09/%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3SSL-TLS%E5%8D%8F%E8%AE%AE%E5%8C%BA%E5%88%AB%E4%B8%8E%E5%8E%9F%E7%90%86/">https://xiaoyue26.github.io/2018/09/26/2018-09/%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3SSL-TLS%E5%8D%8F%E8%AE%AE%E5%8C%BA%E5%88%AB%E4%B8%8E%E5%8E%9F%E7%90%86/</a></p>
<h1 id="现有架构"><a href="#现有架构" class="headerlink" title="现有架构"></a>现有架构</h1><p>请求=&gt;nginx服务器=&gt;后端网站服务</p>
<h1 id="改造思路-原理"><a href="#改造思路-原理" class="headerlink" title="改造思路/原理"></a>改造思路/原理</h1><p>原理上只需要让nginx负责SSL协议的部分即可，不需要动后端网站服务。<br>客户端发送HTTPS请求到nginx服务器，nginx服务器转发HTTP请求到后端网站服务。<br>（封装的思想，上层变动对底层HTTP服务透明）<br><strong>HTTPS中断、TLS中断：</strong><br>nginx直接负责搞定ssl部分，netty等后端服务只需要负责http部分就好了。<br>如果依赖nginx的话，netty的SslHandler什么的都可以废掉了XD</p>
<p>所以我们只需要关心架构中的前半部分:<br>请求=&gt;nginx服务器</p>
<p>再分解一下这部分的话:<br>用户=&gt;浏览器(chrome)==https请求=&gt;nginx服务器</p>
<p><strong>整个架构中我们需要修改的部分:</strong></p>
<ol>
<li>nginx配置. </li>
</ol>
<p>是的，就这么一项。所以改造成本很低。<br>当然了，如果不想花钱买官方CA证书的话，也就是自己弄一个CA, 然后给自己的网站颁发证书的话，还需要改动用户浏览器的信任CA，那么需要修改的部分就增加一项了:</p>
<ol>
<li>nginx配置;</li>
<li>用户浏览器信任CA。 </li>
</ol>
<p>这里的证书、CA是个啥概念呢？<br>证书: 就好比我们网站的身份证；<br>CA  : 就好比派发身份证的派出所。<br>本质上是一个信任传递、担保的过程，用户浏览器会默认信任几个官方的CA，只要官方CA承认的网站，信任传递一下，用户就可以也信任了。<br>参见下图可以通过chrome右键”检查”的<code>security</code>面板查看证书的详细信息。</p>
<img src="/images/2019-04/ca.png" class="" width="800" height="1200" title="ca">

<p>所以如果花钱让官方CA帮我们签发证书的话，用户可以直接默认信任我们的证书；<br>而如果我们自己弄的CA的话，好比自己开的黑作坊，用户不可能直接信任黑作坊签发的身份证的，就需要修改用户浏览器配置了，加入我们的私人CA证书。</p>
<h1 id="公网HTTPS"><a href="#公网HTTPS" class="headerlink" title="公网HTTPS"></a>公网HTTPS</h1><h2 id="生成数字证书"><a href="#生成数字证书" class="headerlink" title="生成数字证书"></a>生成数字证书</h2><p>可以参考:<br><a href="http://www.ruanyifeng.com/blog/2016/08/migrate-from-http-to-https.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2016/08/migrate-from-http-to-https.html</a><br>从<br><a href="https://www.gogetssl.com/" target="_blank" rel="noopener">https://www.gogetssl.com/</a><br><a href="https://www.ssls.com/" target="_blank" rel="noopener">https://www.ssls.com/</a><br><a href="https://sslmate.com/" target="_blank" rel="noopener">https://sslmate.com/</a><br>购买SSL证书。</p>
<p>免费的:<br><a href="https://certbot.eff.org/" target="_blank" rel="noopener">https://certbot.eff.org/</a><br>可以用这个工具，选择转发服务器和操作系统，生成证书:<br><a href="https://certbot.eff.org/lets-encrypt" target="_blank" rel="noopener">https://certbot.eff.org/lets-encrypt</a></p>
<h2 id="配置nginx"><a href="#配置nginx" class="headerlink" title="配置nginx"></a>配置nginx</h2><p>把原来nginx配置中的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listen       80;</span><br></pre></td></tr></table></figure>
<p>改成:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">listen 443 ssl;</span><br><span class="line">ssl_certificate &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;ssl&#x2F;private&#x2F;server.crt;</span><br><span class="line">ssl_certificate_key &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;ssl&#x2F;private&#x2F;device.key;</span><br></pre></td></tr></table></figure>
<p>这里的service.crt就是数字证书了。<br>如果要支持http和https同时可以访问，就把<code>listen 80</code>再加上。</p>
<p>如果要强制https，即使访问了http也强制跳转https(一般都需要这样搞),可以增加rewrite配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name localhost;</span><br><span class="line">        rewrite ^(.*) https:&#x2F;&#x2F;$server_name$1 permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="局域网HTTPS"><a href="#局域网HTTPS" class="headerlink" title="局域网HTTPS"></a>局域网HTTPS</h1><p>公网https起码要买个域名,买个服务器(阿里云),如果只是局域网玩玩、或者自签证书,可以如下操作:</p>
<ol>
<li>本地生成一个CA;</li>
<li>用这个CA给自己网站的数字证书签名，生成网站数字证书;</li>
<li>修改nginx配置;</li>
<li>配置用户chrome信任第一步中的CA。</li>
</ol>
<p>可以看出多了1，2两步来生成证书,代替购买证书;<br>多了第4步来强制用户信任非官方CA.</p>
<h2 id="1-本地生成CA"><a href="#1-本地生成CA" class="headerlink" title="1. 本地生成CA"></a>1. 本地生成CA</h2><p>找个干净的目录开始操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -des3 -out rootCA.key 2048</span><br><span class="line">openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem</span><br></pre></td></tr></table></figure>
<p>里面可以填下密码，email地址和ca的名字。其他的可以留空。</p>
<p>第一条命令: 生成本地CA的密钥<code>rootCA.key</code>(要记住你设置的密码,比如我的是<code>staythenight</code>)；<br>第二条命令: 用这个密钥进行签名，生成一张CA的证书<code>rootCA.pem</code>.<br>(这里设置的过期时间为1024天).</p>
<h2 id="2-生成网站数字证书-用这个CA给自己网站的数字证书签名"><a href="#2-生成网站数字证书-用这个CA给自己网站的数字证书签名" class="headerlink" title="2. 生成网站数字证书(用这个CA给自己网站的数字证书签名)"></a>2. 生成网站数字证书(用这个CA给自己网站的数字证书签名)</h2><p>为了避免chrome的<code>NET::ERR_CERT_COMMON_NAME_INVALID</code>错误，需要在网站证书里填一些额外的信息。<br>首先创建文件<code>server.csr.cnf</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[req]</span><br><span class="line">default_bits &#x3D; 2048</span><br><span class="line">prompt &#x3D; no</span><br><span class="line">default_md &#x3D; sha256</span><br><span class="line">distinguished_name &#x3D; dn</span><br><span class="line"></span><br><span class="line">[dn]</span><br><span class="line">C&#x3D;US</span><br><span class="line">ST&#x3D;RandomState</span><br><span class="line">L&#x3D;RandomCity</span><br><span class="line">O&#x3D;RandomOrganization</span><br><span class="line">OU&#x3D;RandomOrganizationUnit</span><br><span class="line">emailAddress&#x3D;296671657@qq.com # 修改成自己的email</span><br><span class="line">CN &#x3D; kandiandata.oa.com # 修改成自己的域名</span><br></pre></td></tr></table></figure>

<p>然后创建文件<code>v3.ext</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">authorityKeyIdentifier&#x3D;keyid,issuer</span><br><span class="line">basicConstraints&#x3D;CA:FALSE</span><br><span class="line">keyUsage &#x3D; digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment</span><br><span class="line">subjectAltName &#x3D; @alt_names</span><br><span class="line"></span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1 &#x3D; kandiandata.oa.com # 修改成自己的域名</span><br><span class="line">DNS.2 &#x3D; localhost</span><br></pre></td></tr></table></figure>

<p>创建证书:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">openssl req -new -sha256 -nodes -out server.csr -newkey rsa:2048 -keyout device.key -config server.csr.cnf</span><br><span class="line"></span><br><span class="line">openssl x509 -req -in server.csr \</span><br><span class="line">-CA rootCA.pem \</span><br><span class="line">-CAkey rootCA.key \</span><br><span class="line">-CAcreateserial -out server.crt -days 1800 -sha256 -extfile v3.ext</span><br></pre></td></tr></table></figure>
<p>第一条命令: 用<code>server.csr.cnf</code>配置生成网站证书<code>server.csr</code>,同时生成网站私钥<code>device.key</code>(给nginx用的)。<br>第二条命令: 用CA私钥<code>rootCA.key</code>以CA的名义(<code>rootCA.pem</code>)️给网站证书签名，生成CA签名后的证书<code>server.crt</code>，同时加上<code>v3.ext</code>中的配置（防止chrome报错）。</p>
<p>到这里我们就准备好了下一步nginx要用到的两个文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server.crt</span><br><span class="line">device.key</span><br></pre></td></tr></table></figure>
<p><code>server.crt</code>: 网站的数字证书;<br><code>device.key</code>: 网站的私钥，用来解开用户发过来的通信密码。详细原理参见:<br><a href="http://xiaoyue26.github.io/2018/09/26/2018-09/%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3SSL-TLS%E5%8D%8F%E8%AE%AE%E5%8C%BA%E5%88%AB%E4%B8%8E%E5%8E%9F%E7%90%86/">http://xiaoyue26.github.io/2018/09/26/2018-09/%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3SSL-TLS%E5%8D%8F%E8%AE%AE%E5%8C%BA%E5%88%AB%E4%B8%8E%E5%8E%9F%E7%90%86/</a></p>
<h2 id="3-配置nginx"><a href="#3-配置nginx" class="headerlink" title="3. 配置nginx"></a>3. 配置nginx</h2><p>这里和之前的一样,打开ssl支持，监听443:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">listen 443 ssl;</span><br><span class="line">ssl_certificate &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;ssl&#x2F;private&#x2F;server.crt;</span><br><span class="line">ssl_certificate_key &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;ssl&#x2F;private&#x2F;device.key;</span><br></pre></td></tr></table></figure>
<p>加上监听80+重定向:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name localhost;</span><br><span class="line">        rewrite ^(.*) https:&#x2F;&#x2F;$server_name$1 permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="4-配置用户chrome信任第一步中的CA"><a href="#4-配置用户chrome信任第一步中的CA" class="headerlink" title="4. 配置用户chrome信任第一步中的CA"></a>4. 配置用户chrome信任第一步中的CA</h2><p>将第一步中的<code>rootCA.pem</code>发送给用户，让它安装即可。<br>(千万不要发错了。)<br>如果是mac系统，可以直接双击安装到钥匙串中:</p>
<img src="/images/2019-04/rootca_pem.png" class="" width="800" height="1200" title="rootca_pem">
<p>在钥匙串中选择<code>系统</code>=&gt;<code>证书</code>,然后完全信任ca的证书即可:</p>
<img src="/images/2019-04/permit_ca.png" class="" width="800" height="1200" title="permit_ca">

<p>最后得到chrome的承认:</p>
<img src="/images/2019-04/chrome_safe.png" class="" width="800" height="1200" title="chrome_safe">


<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>还可以查看openssl支持的ssl/tls版本:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl ciphers -v | awk &#39;&#123;print $2&#125;&#39; | sort | uniq</span><br></pre></td></tr></table></figure>
<p>查看本地的443端口是否支持tls1.2协商:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl s_client -connect localhost:443 -tls1_2</span><br></pre></td></tr></table></figure>
<p>成功的话会返回一大段内容，包括:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 前面一大堆</span><br><span class="line">---</span><br><span class="line">Certificate chain</span><br><span class="line"> 0 s:&#x2F;C&#x3D;US&#x2F;ST&#x3D;RandomState&#x2F;L&#x3D;RandomCity&#x2F;O&#x3D;RandomOrganization&#x2F;OU&#x3D;RandomOrganizationUnit&#x2F;emailAddress&#x3D;296671657@qq.com&#x2F;CN&#x3D;kandiandata.oa.com</span><br><span class="line">   i:&#x2F;C&#x3D;XX&#x2F;L&#x3D;Default City&#x2F;O&#x3D;tencent&#x2F;OU&#x3D;kandian&#x2F;CN&#x3D;pipe_ca&#x2F;emailAddress&#x3D;296671657@qq.com</span><br><span class="line">---</span><br><span class="line">Server certificate</span><br><span class="line"># 后面一大堆</span><br></pre></td></tr></table></figure>
<p>失败的话:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">no peer certificate available</span><br><span class="line">---</span><br><span class="line">No client certificate CA names sent</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/04/14/2019-04/2019%E5%B9%B4%E7%9A%84%E4%BA%BA%E4%BB%AC%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90HTTPS%E8%AF%81%E4%B9%A6/" data-id="ck96cxppn00i2maamg3y52508" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/https/" rel="tag">https</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nginx/" rel="tag">nginx</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ssl/" rel="tag">ssl</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-2019-03/mysql统计信息更新" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/31/2019-03/mysql%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0/" class="article-date">
  <time datetime="2019-03-31T11:38:49.000Z" itemprop="datePublished">2019-03-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/mysql/">mysql</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/31/2019-03/mysql%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0/">mysql统计信息更新</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <blockquote>
<p>本文只关注innodb。 </p>
</blockquote>
<p>mysql优化器选择执行计划的时候需要依据一定的采样统计信息，不然对数据完全不了解的话，就无法选择成本低的执行计划了。</p>
<p>统计信息的配置有以下几个自由度:</p>
<ol>
<li>是否持久化;</li>
<li>更新统计信息的时机;</li>
<li>采样多少个page。 </li>
</ol>
<h1 id="是否持久化"><a href="#是否持久化" class="headerlink" title="是否持久化"></a>是否持久化</h1><p>采样统计信息可以有两种选择：</p>
<ol>
<li>持久化: 默认是持久化，也就是存磁盘。</li>
<li>非持久化.</li>
</ol>
<p>控制的选项:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show variables like &#39;%innodb_stats_persistent%&#39;;</span><br></pre></td></tr></table></figure>
<p>默认是on，也就是持久化。<br>具体存哪里呢，主要是存mysql库和information_schema库下(5.6.x):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INFORMATION_SCHEMA.TABLES</span><br><span class="line">INFORMATION_SCHEMA.STATISTICS</span><br><span class="line">mysql.innodb_table_stats</span><br><span class="line">mysql.innodb_index_stats</span><br></pre></td></tr></table></figure>

<h1 id="更新统计信息的时机"><a href="#更新统计信息的时机" class="headerlink" title="更新统计信息的时机"></a>更新统计信息的时机</h1><p>相关参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">innodb_stats_on_metadata: 是否每次都重新计算统计信息(配合非持久化使用),默认off;</span><br><span class="line">innodb_stats_auto_recalc: 插入数据量超过原表10%的时候更新统计信息,默认on。</span><br></pre></td></tr></table></figure>
<p>总结一下mysql更新统计信息的时机:</p>
<ol>
<li>手动运行触发语句如<code>analyze table xx</code>的时候;</li>
<li>如果<code>innodb_stats_auto_recalc</code>为on: 插入数据量超过原表10%的时候更新统计信息;</li>
<li>如果<code>innodb_stats_on_metadata</code>为on: 每次查询schema.table表的是更新统计信息(一般不开启，性能太差)。</li>
</ol>
<h1 id="采样page数量"><a href="#采样page数量" class="headerlink" title="采样page数量"></a>采样page数量</h1><p>相关参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">| innodb_stats_sample_pages            | 8     |</span><br><span class="line">| innodb_stats_persistent_sample_pages | 20    |</span><br><span class="line">| innodb_stats_transient_sample_pages  | 8     |</span><br></pre></td></tr></table></figure>
<p><code>innodb_stats_sample_pages</code>废弃改成了<code>innodb_stats_persistent_sample_pages</code>和<code>innodb_stats_transient_sample_pages</code>,灵活控制持久化和非持久化下的采样page数。</p>
<p>可以看出默认情况持久化采样20个page。 </p>
<h2 id="单表配置"><a href="#单表配置" class="headerlink" title="单表配置"></a>单表配置</h2><p>上述所有都是全局配置，还可以为每个表单独3个参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">STATS_PERSISTENT  : 1: 持久化统计信息;</span><br><span class="line">STATS_AUTO_RECALC : 超过10%更新统计信息。</span><br><span class="line">STATS_SAMPLE_PAGES: 采样页数。</span><br></pre></td></tr></table></figure>
<p>可以看出为每个表设置的参数依然是这3个自由度: 是否持久化、更新统计信息时机、采样页数。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaoyue26.github.io/2019/03/31/2019-03/mysql%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0/" data-id="ck96cxppi00hhmaam20j0cn5z" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/" rel="tag">mysql</a></li></ul>

    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/6/">&amp;laquo; 上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/8/">下一页&amp;raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    

  
    
  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/09/24/2023-09/%E6%97%B6%E9%92%9F%E5%90%8C%E6%AD%A5%E5%8F%8A%E4%B8%80%E8%87%B4%E6%80%A7/">时钟同步及一致性</a>
          </li>
        
          <li>
            <a href="/2023/02/03/2023-02/%E8%B0%83%E4%BC%98-ByteString%E7%9B%B8%E5%85%B3%E5%86%85%E5%AD%98%E6%8B%B7%E8%B4%9D%E9%97%AE%E9%A2%98/">调优-ByteString相关内存拷贝问题</a>
          </li>
        
          <li>
            <a href="/2023/02/01/2023-02/%E8%AF%8A%E6%96%AD-jvm%E7%A8%8B%E5%BA%8Fcpu%E9%97%AE%E9%A2%98/">诊断-jvm程序cpu问题</a>
          </li>
        
          <li>
            <a href="/2022/11/21/2022-11/cpu%E6%AF%9B%E5%88%BA%E9%97%AE%E9%A2%98%E9%80%9A%E7%94%A8%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF/">调优-cpu毛刺问题通用优化思路</a>
          </li>
        
          <li>
            <a href="/2022/10/03/2022-10/java%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%94%BB%E5%87%BB-md/">java反序列化攻击</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2023 风梦七&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;296671657@qq.com
    </div>
  </div>
</footer>
 
<script src="/jquery/jquery.min.js"></script>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid begin -->

<!-- author:forvoid end -->


   <script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>

  <script>
    ((window.gitter = {}).chat = {}).options = {
      //room替换成自己的聊天室名称即可，room的名称规则是：username/roomname
      room: 'xiaoyue26/comment'
    };
  </script>


<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 
<script src="/js/is.js"></script>



  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


<script src="/js/elevator.js"></script>

  </div>
</body>
</html>